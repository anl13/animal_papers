## Animal Paper Collection (Ongoing)
[2023.12.14] Here is a workshop involving animals and infants [CV4Smalls Workshop](https://cv4smalls.sites.northeastern.edu/accepted-submissions/)! Feel free to check it out!

[2023.11.26] My own paper "MAMMAL" is published online in Nature Communications now! 

[2023.06.10] The program of [CV4Animals Workshop in CVPR 2023](https://www.cv4animals.com/2023-accepted-papers) is available!

[2022.08.08] Here is the Ph.D thesis of SEMIH GÜNEL (https://infoscience.epfl.ch/record/293821), who is the author of well-known DeepFly3D and LiftPose3D.

[2022.06.22] The 2nd CV4Animals Workshop in CVPR 2022 (https://www.cv4animals.com/) presents many excellent works on rodent 3D reconstruction!
  
[2022.01.02] To make it easy to track, I sort the papers using the timepoint I find them (not publication time), and add label badges to show paper features. 

[2021.11.28] I recommend [MMPose](https://github.com/open-mmlab/mmpose) for 2D animal pose estimation. It has collected various kinds of [datasets](https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_animal_keypoint.md).

[2021.07.16] I recommend CV4Animal workshop in CVPR 2021 (https://www.cv4animals.com/) because it is a good collection of recent advances in animal pose estimation area! 

Recently, markerless animal **motion capture** and **3D reconstruction** attracts more and more attention in computer vision community. Inspired by remarkable techniques for markerless human motion capture, a few excellent literatures appear for animal modeling and reconstruction such as [SMAL](http://smal.is.tue.mpg.de/
) and [DeepLabCut](http://www.mousemotorlab.org/deeplabcut). However, there is still a long way before computer vision methods could capture natural motion of arbitary animals in industrial-grade. 

Therefore, I contribute this repository to track every step towards the ultimate goal of high quality animal capture. If you want to add/remove an article, please send an email to [Liang An](https://anl13.github.io/)(al17 at mails dot tsinghua dot edu dot cn). Thank all the authors for their contribution and support.


<br>

<table><tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Tsang2024.jpg"/></a></td>
<td align="left" width=550><em>Investigating the use of odour and colour foraging cues by rosy-faced lovebirds (Agapornis roseicollis) using deep-learning based behavioural analysis</em><br>
<font size=2.5>Winson King Wai Tsang</font>, 
<font size=2.5>Emily Shui Kei Poon</font>, 
<font size=2.5>Chris Newman</font>, 
<font size=2.5>Christina D. Buesching</font>, 
<font size=2.5>Simon Yung Wa Sin</font><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.18.580921v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Xing2024.jpg"/></a></td>
<td align="left" width=550><em>Automated 3D analysis of social head-gaze behaviors in freely moving marmosets</em><br>
<font size=2.5>Feng Xing</font>, 
<font size=2.5>Alec G. Sheffield</font>, 
<font size=2.5>Monika P. Jadi</font>, 
<font size=2.5>Steve W.C Chang</font>, 
<a href="https://medicine.yale.edu/profile/anirvan-nandy/"><font size=2.5>Anirvan S. Nandy</font></a><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.16.580693v1.full.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/YttriLab/A-SOID"><img src="teasers/Schweihoff2022.jpg"/></a></td>
<td align="left" width=550><em>A-SOiD, an active learning platform for expert-guided, data efficient discovery of behavior</em><br>
<font size=2.5>Jens F. Tillmann</font>, 
<font size=2.5>Alexander I. Hsu</font>, 
<font size=2.5>Martin K. Schwarz</font>, 
<a href="https://www.cmu.edu/bio/people/faculty/yttri.html"><font size=2.5>Eric A. Yttri</font></a><br>
<font size=2.5>In Nature Methods 2024 </font><br>
<a href="https://www.nature.com/articles/s41592-024-02200-1"><img src="data/paper.png"></a> 
<a href="https://github.com/YttriLab/A-SOID"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Tang2024.jpg"/></a></td>
<td align="left" width=550><em>Anti-drift pose tracker (ADPT): A transformer-based network for robust animal pose estimation cross-species</em><br>
<font size=2.5>Guoling Tang</font>, 
<font size=2.5>Yaning Han</font>, 
<font size=2.5>Quanying Liu</font>, 
<font size=2.5>Pengfei Wei</font><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.06.579164v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Azechi2024.jpg"/></a></td>
<td align="left" width=550><em>vmTracking: Virtual Markers Overcome Occlusion and Crowding in Multi-Animal Pose Tracking</em><br>
<font size=2.5>Hirotsugu Azechi</font>, 
<font size=2.5>Susumu Takahashi</font><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.07.579241v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Chen2024.jpg"/></a></td>
<td align="left" width=550><em>ARBUR, a machine learning-based analysis system for relating behaviors and ultrasonic vocalizations of rats</em><br>
<font size=2.5>Zhe Chen</font>, 
<font size=2.5>Guanglu Jia</font>, 
<font size=2.5>Qijie Zhou</font>, 
<font size=2.5>Yulai Zhang</font>, 
<font size=2.5>Zhenzhen Quan</font>, 
<font size=2.5>Xuechao Chen</font>, 
<font size=2.5>Toshio Fukuda</font>, 
<font size=2.5>Qiang Huang</font>, 
<a href="https://www.researchgate.net/profile/Qing-Shi-16"><font size=2.5>Qing Shi</font></a><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.12.19.572288v2.full.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Muramatsu2024.jpg"/></a></td>
<td align="left" width=550><em>WILDPOSE: A LONG-RANGE 3D WILDLIFE MOTION CAPTURE SYSTEM</em><br>
<font size=2.5>Naoya Muramatsu</font>, 
<font size=2.5>Sangyun Shin</font>, 
<font size=2.5>Qianyi Deng</font>, 
<font size=2.5>Andrew Markham</font>, 
<a href="https://www.africanroboticsunit.com/"><font size=2.5>Amir Patel</font></a><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.05.578861v2.full.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://anonymous.4open.science/r/2489/README.md"><img src="teasers/Ogawa2024.jpg"/></a></td>
<td align="left" width=550><em>Combining Unity with machine vision to create low latency, flexible, and simple virtual realities</em><br>
<font size=2.5>Yuri Ogawa</font>, 
<font size=2.5>Raymond Aoukar</font>, 
<font size=2.5>Richard Leibbrandt</font>, 
<font size=2.5>Jake S Manger</font>, 
<font size=2.5>Zahra M Bagheri</font>, 
<font size=2.5>Luke Turnbull</font>, 
<font size=2.5>Chris Johnston</font>, 
<font size=2.5>Pavan K Kaushik</font>, 
<font size=2.5>Jan M Hemmi</font>, 
<a href="https://hoverflyvision.weebly.com/"><font size=2.5>Karin Nordström</font></a><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.05.579029v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://anonymous.4open.science/r/2489/README.md"><img src="data/project.png"></a>
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Martini2024.jpg"/></a></td>
<td align="left" width=550><em>MacAction: Realistic 3D macaque body animation based on multi-camera markerless motion capture</em><br>
<font size=2.5>Lucas M. Martini</font>, 
<font size=2.5>Anna Bogn´ar</font>, 
<font size=2.5>Rufin Vogels</font>, 
<font size=2.5>Martin A. Giese</font><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.01.29.577734v1.full.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kaul2024.jpg"/></a></td>
<td align="left" width=550><em>DAMM for the detection and tracking of multiple animals within complex social and environmental settings</em><br>
<font size=2.5>Gaurav Kaul</font>, 
<font size=2.5>Jonathan McDevitt</font>, 
<font size=2.5>Justin Johnson</font>, 
<font size=2.5>Ada Eban-Rothschild</font><br>
<font size=2.5>In biorxiv 2024 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2024.01.18.576153v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Vogg2024.jpg"/></a></td>
<td align="left" width=550><em>Computer Vision for Primate Behavior Analysis in the Wild</em><br>
<font size=2.5>Richard Vogg</font>, 
<font size=2.5>Timo Luddecke</font>, 
<font size=2.5>Jonathan Henrich</font>, 
<font size=2.5>Sharmita Dey</font>, 
<font size=2.5>Matthias Nuske</font>, 
<font size=2.5>Valentin Hassler</font>, 
<font size=2.5>Derek Murphy</font>, 
<font size=2.5>Julia Fischer</font>, 
<font size=2.5>Julia Ostner</font>, 
<font size=2.5>Oliver Schulke</font>, 
<font size=2.5>Peter M. Kappeler</font>, 
<font size=2.5>Claudia Fichte</font>, 
<font size=2.5>Alexander Gail</font>, 
<font size=2.5>Stefan Treue</font>, 
<font size=2.5>Hansjorg Scherberger</font>, 
<font size=2.5>Florentin Worg otter</font>, 
<a href="https://eckerlab.org/"><font size=2.5>Alexander S. Ecker</font></a><br>
<font size=2.5>In arxiv 2024 </font><br>
<a href="https://arxiv.org/pdf/2401.16424.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-primate-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-primate-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://obrookes.github.io/panaf.github.io/"><img src="teasers/brookes2024.jpg"/></a></td>
<td align="left" width=550><em>PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition</em><br>
<font size=2.5>Otto Brookes et.al.</font><br>
<font size=2.5>In IJCV 2024 </font><br>
<a href="https://arxiv.org/pdf/2401.13554.pdf"><img src="data/paper.png"></a> 
<a href="https://obrookes.github.io/panaf.github.io/"><img src="data/project.png"></a>
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Han2023.jpg"/></a></td>
<td align="left" width=550><em>Multi-animal 3D social pose estimation, identification and behaviour embedding with a few-shot learning framework</em><br>
<a href="https://loop.frontiersin.org/people/2018185/overview"><font size=2.5>Yaning Han</font></a>, 
<font size=2.5>Ke Chen</font>, 
<font size=2.5>Yunke Wang</font>, 
<font size=2.5>Wenhao Liu</font>, 
<font size=2.5>Zhouwei Wang</font>, 
<font size=2.5>Xiaojing Wang</font>, 
<font size=2.5>Chuanliang Han</font>, 
<font size=2.5>Jiahui Liao</font>, 
<font size=2.5>Kang Huang</font>, 
<font size=2.5>Shengyuan Cai</font>, 
<font size=2.5>Yiting Huang</font>, 
<font size=2.5>Nan Wang</font>, 
<font size=2.5>Jinxiu Li</font>, 
<font size=2.5>Yangwangzi Song</font>, 
<font size=2.5>Jing Li</font>, 
<font size=2.5>Guodong Wang</font>, 
<a href="http://wanglab.siat.ac.cn/wanglab_en/index.php?a=publications"><font size=2.5>Liping Wang</font></a>, 
<font size=2.5>Yaping Zhang</font>, 
<a href="http://bcbdi.siat.ac.cn/index.php/member2/showMember/nid/92.shtml"><font size=2.5>Pengfei Wei</font></a><br>
<font size=2.5>In Nature Machine Intelligence 2024 </font><br>
<a href="https://www.nature.com/articles/s42256-023-00776-5"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/MouseLand/rastermap"><img src="teasers/Syeda2023.jpg"/></a></td>
<td align="left" width=550><em>Facemap: a framework for modeling neural activity based on orofacial tracking</em><br>
<font size=2.5>Atika Syeda</font>, 
<font size=2.5>Lin Zhong</font>, 
<font size=2.5>Renee Tung</font>, 
<font size=2.5>Will Long</font>, 
<font size=2.5>Marius Pachitariu</font>, 
<a href="https://mouseland.github.io/"><font size=2.5>Carsen Stringer</font></a><br>
<font size=2.5>In Nature Neuroscience 2023 </font><br>
<a href="https://www.nature.com/articles/s41593-023-01490-6"><img src="data/paper.png"></a> 
<a href="https://github.com/MouseLand/rastermap"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/An2023.jpg"/></a></td>
<td align="left" width=550><em>Three-dimensional surface motion capture of multiple freely moving pigs using MAMMAL</em><br>
<a href="http://anl13.github.io/"><font size=2.5>Liang An</font></a>, 
<font size=2.5>Jilong Ren</font>, 
<a href="http://ytrock.com/"><font size=2.5>Tao Yu</font></a>, 
<font size=2.5>Tang Hai</font>, 
<a href="https://www.jialabtsinghua.com/"><font size=2.5>Yichang Jia</font></a>, 
<a href="https://www.liuyebin.com/"><font size=2.5>Yebin Liu</font></a><br>
<font size=2.5>In Nature Communications 2023 </font><br>
<a href="https://www.nature.com/articles/s41467-023-43483-w"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-pig-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-pig-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Hayakawa2023.jpg"/></a></td>
<td align="left" width=550><em>DeepLabCut-based Behavioural and Posture Analysis in a Cricket</em><br>
<font size=2.5>Shota Hayakawa</font>, 
<font size=2.5>Kosuke Kataoka</font>, 
<font size=2.5>Masanobu Yamamoto</font>, 
<font size=2.5>Toru Asahi</font>, 
<a href="https://web.tuat.ac.jp/~tszk/publications.html"><font size=2.5>Takeshi Suzuki</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://doi.org/10.1101/2023.11.14.567118"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Mitelut2023.jpg"/></a></td>
<td align="left" width=550><em>Distinct developmental trajectories of autonomous behaviors and agency in rodents</em><br>
<font size=2.5>C Mitelut</font>, 
<font size=2.5>M Diez Castro</font>, 
<font size=2.5>RE Peterson</font>, 
<font size=2.5>M Gonçalves</font>, 
<font size=2.5>MM Gamer</font>, 
<font size=2.5>SRO Nilsson</font>, 
<font size=2.5>TD Pereira</font>, 
<font size=2.5>DH Sanes</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.11.10.566632v1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-gerbil-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-gerbil-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Yang2023-2.jpg"/></a></td>
<td align="left" width=550><em>BARN: Behavior-Aware Relation Network for multi-label behavior detection in socially housed macaques</em><br>
<font size=2.5>Sen Yang</font>, 
<font size=2.5>Zhi-Yuan Chen</font>, 
<font size=2.5>Ke-Wei Liang</font>, 
<font size=2.5>Cai-Jie Qin</font>, 
<font size=2.5>Yang Yang</font>, 
<font size=2.5>Wen-Xuan Fan</font>, 
<font size=2.5>Chen-Lu Jie</font>, 
<font size=2.5>Xi-Bo Ma</font><br>
<font size=2.5>In Zoological Research 2023 </font><br>
<a href="https://www.zoores.ac.cn/en/article/doi/10.24272/j.issn.2095-8137.2022.485"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Haalck2023.jpg"/></a></td>
<td align="left" width=550><em>CATER: Combined Animal Tracking & Environment Reconstruction</em><br>
<font size=2.5>Lars Haalck</font>, 
<font size=2.5>Michael Mangan</font>, 
<font size=2.5>Antoine Wystrach</font>, 
<font size=2.5>Leo Clement</font>, 
<font size=2.5>Barbara Webb</font>, 
<a href="https://www.uni-muenster.de/Geoinformatics.cvmls/people/risse.shtml"><font size=2.5>Benjamin Risse</font></a><br>
<font size=2.5>In Science Advances 2023 </font><br>
<a href="https://www.science.org/doi/10.1126/sciadv.adg2094"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-ecology-orange" align="bottom"><img src="https://img.shields.io/badge/topic-ecology-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Nagy2023-2.jpg"/></a></td>
<td align="left" width=550><em>SMART-BARN: Scalable multimodal arena for real-time tracking behavior of animals in large numbers</em><br>
<font size=2.5>Máté Nagy</font>, 
<font size=2.5>Hemal Naik</font>, 
<font size=2.5>Fumihiro Kano</font>, 
<font size=2.5>Nora V. Carlson</font>, 
<font size=2.5>Jens C. Koblitz</font>, 
<font size=2.5>Martin Wikelski</font>, 
<a href="https://www.ab.mpg.de/couzin"><font size=2.5>Iain D. Couzin</font></a><br>
<font size=2.5>In Science Advances 2023 </font><br>
<a href="https://www.science.org/doi/10.1126/sciadv.adf8068"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Voloh2021.jpg"/></a></td>
<td align="left" width=550><em>Hierarchical organization of rhesus macaque behavior</em><br>
<font size=2.5>Benjamin Voloh</font>, 
<font size=2.5>Benjamin R. Eisenreich</font>, 
<font size=2.5>David J-N. Maisson</font>, 
<font size=2.5>R. Becket Ebitz</font>, 
<font size=2.5>Hyun Soo Park</font>, 
<font size=2.5>Benjamin Y. Hayden</font>, 
<font size=2.5>Jan Zimmermann</font><br>
<font size=2.5>In Oxford Open Neuroscience 2023 </font><br>
<a href="https://doi.org/10.1093/oons/kvad006"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Luxem2023.jpg"/></a></td>
<td align="left" width=550><em>Open-source tools for behavioral video analysis: Setup, methods, and best practices</em><br>
<font size=2.5>Kevin Luxem</font>, 
<font size=2.5>Jennifer J Sun</font>, 
<font size=2.5>Sean P Bradley</font>, 
<font size=2.5>Keerthi Krishnan</font>, 
<font size=2.5>Eric Yttri</font>, 
<font size=2.5>Jan Zimmermann</font>, 
<a href="https://talmopereira.com/"><font size=2.5>Talmo D Pereira</font></a>, 
<a href="https://www.american.edu/cas/faculty/laubach.cfm"><font size=2.5>Mark Laubach</font></a><br>
<font size=2.5>In Elife 2023 </font><br>
<a href="https://elifesciences.org/articles/79305"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Luo2023.jpg"/></a></td>
<td align="left" width=550><em>Automated measurement of livestock body based on pose normalisation using statistical shape model</em><br>
<font size=2.5>Xinying Luo</font>, 
<font size=2.5>Yihu Hu</font>, 
<font size=2.5>Zicheng Gao</font>, 
<font size=2.5>Hao Guo</font>, 
<font size=2.5>Yang Su</font><br>
<font size=2.5>In Biosystems Engineering 2023 </font><br>
<a href="https://www.sciencedirect.com/science/article/pii/S1537511023000211"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-pig-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-pig-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kaneko2023.jpg"/></a></td>
<td align="left" width=550><em>Establishing an AI-based evaluation system that quantifies social/pathophysiological behaviors of common marmosets</em><br>
<font size=2.5>Takaaki Kaneko</font>, 
<font size=2.5>Jumpei Matsumoto</font>, 
<font size=2.5>Wanyi Lu</font>, 
<font size=2.5>Xincheng Zhao</font>, 
<font size=2.5>Louie Richard UenoNigh</font>, 
<font size=2.5>Takao Oishi</font>, 
<font size=2.5>Kei Kimura</font>, 
<font size=2.5>Yukiko Otsuka</font>, 
<font size=2.5>Andi Zheng</font>, 
<font size=2.5>Kensuke Ikenaka</font>, 
<font size=2.5>Kousuke Baba</font>, 
<font size=2.5>Hideki Mochizuki</font>, 
<font size=2.5>Hisao Nishijo</font>, 
<font size=2.5>Ken-ichi Inoue</font>, 
<font size=2.5>Masahiko Takada</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.10.16.561623v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Nagy2023.jpg"/></a></td>
<td align="left" width=550><em>Long-term tracking of social structure in groups of rats</em><br>
<font size=2.5>M´at´e Nagy</font>, 
<font size=2.5>Jacob D. Davidson</font>, 
<font size=2.5>G´abor V´as´arhelyi</font>, 
<font size=2.5>D´aniel Abel</font>, 
<font size=2.5>Enik˝o Kubinyi</font>, 
<font size=2.5>Ahmed El Hady</font>, 
<font size=2.5>Tam´as Vicsek</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.03.18.533183v3.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://zenodo.org/record/8013401"><img src="teasers/Bordes2023.jpg"/></a></td>
<td align="left" width=550><em>Automatically annotated motion tracking identifies a distinct social behavioral profile following chronic social defeat stress</em><br>
<font size=2.5>Joeri Bordes</font>, 
<font size=2.5>Lucas Miranda</font>, 
<font size=2.5>Maya Reinhardt</font>, 
<font size=2.5>Sowmya Narayan</font>, 
<font size=2.5>Jakob Hartmann</font>, 
<font size=2.5>Emily L. Newman</font>, 
<font size=2.5>Lea Maria Brix</font>, 
<font size=2.5>Lotte van Doeselaar</font>, 
<font size=2.5>Clara Engelhardt</font>, 
<font size=2.5>Larissa Dillmann</font>, 
<font size=2.5>Shiladitya Mitra</font>, 
<font size=2.5>Kerry J. Ressler</font>, 
<font size=2.5>Benno Pütz</font>, 
<font size=2.5>Felix Agakov</font>, 
<font size=2.5>Bertram Müller-Myhsok</font>, 
<font size=2.5>Mathias V. Schmidt</font><br>
<font size=2.5>In Nature Communications 2023 </font><br>
<a href="https://www.nature.com/articles/s41467-023-40040-3"><img src="data/paper.png"></a> 
<a href="https://zenodo.org/record/8013401"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://lote-animal.github.io/"><img src="teasers/Liu2023.jpg"/></a></td>
<td align="left" width=550><em>LoTE-Animal: A Long Time-span Dataset for Endangered Animal Behavior Understanding</em><br>
<font size=2.5>Dan Liu</font>, 
<font size=2.5>Jin Hou</font>, 
<font size=2.5>Shaoli Huang</font>, 
<font size=2.5>Jing Liu</font>, 
<font size=2.5>Yuxin He</font>, 
<font size=2.5>Bochuan Zheng</font>, 
<a href="https://cie.nwsuaf.edu.cn/szdw/js/2008116162/index.htm"><font size=2.5>Jifeng Ning</font></a>, 
<font size=2.5>Jindong Zhang</font><br>
<font size=2.5>In ICCV 2023 </font><br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_LoTE-Animal_A_Long_Time-span_Dataset_for_Endangered_Animal_Behavior_Understanding_ICCV_2023_paper.html"><img src="data/paper.png"></a> 
<a href="https://lote-animal.github.io/"><img src="data/project.png"></a>
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Bogna2023.jpg"/></a></td>
<td align="left" width=550><em>Stimulatory effect of monoacylglycerol lipase inhibitor MJN110 on locomotion and step kinematics demonstrated by high-precision 3D motion capture in mice</em><br>
<font size=2.5>Bogna M. Ignatowska-Jankowska</font>, 
<font size=2.5>Aysen Gurkan Ozer</font>, 
<font size=2.5>Alexander Kuck</font>, 
<font size=2.5>Micah J. Niphakis</font>, 
<font size=2.5>Daisuke Ogasawara</font>, 
<font size=2.5>Benjamin F. Cravatt</font>, 
<font size=2.5>Marylka Y. Uusisaari</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.06.25.546437v2.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Chen2023-1.jpg"/></a></td>
<td align="left" width=550><em>NeuroMechFly 2.0, a framework for simulating embodied sensorimotor control in adult Drosophila</em><br>
<font size=2.5>Sibo Wang-Chen</font>, 
<font size=2.5>Victor Alfred Stimpfling</font>, 
<font size=2.5>Pembe Gizem Ozdil</font>, 
<font size=2.5>Louise Genoud</font>, 
<font size=2.5>Femke Hurtak</font>, 
<font size=2.5>Pavan Ramdya</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.09.18.556649v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/John2023.jpg"/></a></td>
<td align="left" width=550><em>Simultaneous recording of ultrasonic vocalizations and sniffing from socially interacting individual rats using a miniature microphone</em><br>
<font size=2.5>Shanah Rachel John</font>, 
<font size=2.5>Rishika Tiwari</font>, 
<font size=2.5>Yizhaq Goussha</font>, 
<font size=2.5>Rotem Amar</font>, 
<font size=2.5>Alex Bizer</font>, 
<font size=2.5>Shai Netser</font>, 
<font size=2.5>Shlomo Wagner</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.09.02.556068v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Lapp2023.jpg"/></a></td>
<td align="left" width=550><em>Automated Maternal Behavior during Early life in Rodents (AMBER) pipeline</em><br>
<font size=2.5>Hannah E. Lapp</font>, 
<font size=2.5>Melissa G. Salazar</font>, 
<font size=2.5>Frances A. Champagne</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.09.15.557946v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Matsumoto2023.jpg"/></a></td>
<td align="left" width=550><em>Three-dimensional markerless motion capture of multiple freely behaving monkeys for automated characterization of social behavior</em><br>
<font size=2.5>Jumpei Matsumoto</font>, 
<font size=2.5>Takaaki Kaneko</font>, 
<font size=2.5>Kei Kimura</font>, 
<font size=2.5>Salvador Blanco Negrete</font>, 
<font size=2.5>Jia Guo</font>, 
<font size=2.5>Naoko Suda-Hashimoto</font>, 
<font size=2.5>Akihisa Kaneko</font>, 
<font size=2.5>Mayumi Morimoto</font>, 
<font size=2.5>Hiroshi Nishimaru</font>, 
<font size=2.5>Tsuyoshi Setogawa</font>, 
<font size=2.5>Yasuhiro Go</font>, 
<font size=2.5>Tomohiro Shibata</font>, 
<font size=2.5>Hisao Nishijo</font>, 
<font size=2.5>Masahiko Takada</font>, 
<font size=2.5>Ken-ichi Inoue</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.09.13.556332v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://google.github.io/mouse-pose-analysis-dataset/"><img src="teasers/Hu2023.jpg"/></a></td>
<td align="left" width=550><em>3D mouse pose from single‑view video and a new dataset</em><br>
<font size=2.5>Bo Hu</font>, 
<font size=2.5>Bryan Seybold</font>, 
<font size=2.5>Shan Yang</font>, 
<font size=2.5>Avneesh Sud</font>, 
<font size=2.5>Yi Liu</font>, 
<font size=2.5>Karla Barron</font>, 
<font size=2.5>Paulyn Cha</font>, 
<font size=2.5>Marcelo Cosino</font>, 
<font size=2.5>Ellie Karlsson</font>, 
<font size=2.5>Janessa Kite</font>, 
<font size=2.5>Ganesh Kolumam</font>, 
<font size=2.5>Joseph Preciado</font>, 
<font size=2.5>José Zavala‑Solorio</font>, 
<font size=2.5>Chunlian Zhang</font>, 
<font size=2.5>Xiaomeng Zhang</font>, 
<font size=2.5>Martin Voorbach</font>, 
<font size=2.5>Ann E.Tovcimak</font>, 
<font size=2.5>J.Graham Ruby</font>, 
<font size=2.5>David A. Ross</font><br>
<font size=2.5>In Scientific Reports 2023 </font><br>
<a href="https://www.nature.com/articles/s41598-023-40738-w"><img src="data/paper.png"></a> 
<a href="https://google.github.io/mouse-pose-analysis-dataset/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://xujiacong.github.io/Animal3D/"><img src="teasers/Xu2023.jpg"/></a></td>
<td align="left" width=550><em>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</em><br>
<a href="https://xujiacong.github.io/"><font size=2.5>Jiacong Xu</font></a>, 
<font size=2.5>Yi Zhang</font>, 
<font size=2.5>Jiawei Peng</font>, 
<font size=2.5>Wufei Ma</font>, 
<font size=2.5>Artur Jesslen</font>, 
<font size=2.5>Pengliang Ji</font>, 
<font size=2.5>Qixin Hu</font>, 
<font size=2.5>Jiehua Zhang</font>, 
<font size=2.5>Qihao Liu</font>, 
<font size=2.5>Jiahao Wang</font>, 
<font size=2.5>Wei Ji</font>, 
<font size=2.5>Chen Wang</font>, 
<font size=2.5>Xiaoding Yuan</font>, 
<font size=2.5>Prakhar Kaushik</font>, 
<font size=2.5>Guofeng Zhang</font>, 
<font size=2.5>Jie Liu</font>, 
<font size=2.5>Yushan Xie</font>, 
<font size=2.5>Yawen Cui</font>, 
<a href="https://www.cs.jhu.edu/~ayuille/"><font size=2.5>Alan Yuille</font></a>, 
<font size=2.5>Adam Kortylewski</font><br>
<font size=2.5>In ICCV 2023 </font><br>
<a href="https://arxiv.org/pdf/2308.11737.pdf"><img src="data/paper.png"></a> 
<a href="https://xujiacong.github.io/Animal3D/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://gengshan-y.github.io/ppr/"><img src="teasers/Yang2023.jpg"/></a></td>
<td align="left" width=550><em>PPR: Physically Plausible Reconstruction from Monocular Videos</em><br>
<font size=2.5>Gengshan Yang</font>, 
<font size=2.5>Shuo Yang</font>, 
<font size=2.5>John Z. Zhang</font>, 
<font size=2.5>Zachary Manchester</font>, 
<font size=2.5>Deva Ramanan</font><br>
<font size=2.5>In ICCV 2023 </font><br>
<a href="https://gengshan-y.github.io/ppr/PPR.pdf"><img src="data/paper.png"></a> 
<a href="https://gengshan-y.github.io/ppr/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/AdaptiveMotorControlLab/AmadeusGPT"><img src="teasers/Ye2023-2.jpg"/></a></td>
<td align="left" width=550><em>AmadeusGPT: a natural language interface for interactive animal behavioral analysis</em><br>
<a href="https://yeshaokai.github.io/"><font size=2.5>Shaokai Ye</font></a>, 
<font size=2.5>Jessy Lauer</font>, 
<font size=2.5>Mu Zhou</font>, 
<font size=2.5>Alexander Mathis</font>, 
<font size=2.5>Mackenzie Weygandt Mathis</font><br>
<font size=2.5>In NeurIPS 2023 </font><br>
<a href="https://arxiv.org/pdf/2307.04858.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/AdaptiveMotorControlLab/AmadeusGPT"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://chhankyao.github.io/artic3d/"><img src="teasers/Yao2023.jpg"/></a></td>
<td align="left" width=550><em>ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections</em><br>
<font size=2.5>Chun-Han Yao</font>, 
<font size=2.5>Amit Raj</font>, 
<font size=2.5>Wei-Chih Hung</font>, 
<font size=2.5>Yuanzhen Li</font>, 
<font size=2.5>Michael Rubinstein</font>, 
<font size=2.5>Ming-Hsuan Yang</font>, 
<a href="https://varunjampani.github.io/"><font size=2.5>Varun Jampani</font></a><br>
<font size=2.5>In arxiv 2023 </font><br>
<a href="https://arxiv.org/pdf/2306.04619.pdf"><img src="data/paper.png"></a> 
<a href="https://chhankyao.github.io/artic3d/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/bartulem/KISN-PyLab"><img src="teasers/Mimica2023.jpg"/></a></td>
<td align="left" width=550><em>Behavioral decomposition reveals rich encoding structure employed across neocortex in rats</em><br>
<font size=2.5>Bartul Mimica</font>, 
<font size=2.5>Tuçe Tombaz</font>, 
<font size=2.5>Claudia Battistin</font>, 
<font size=2.5>Jingyi Guo Fuglstad</font>, 
<font size=2.5>Benjamin A. Dunn</font>, 
<font size=2.5>Jonathan R. Whitlock</font><br>
<font size=2.5>In Nature Communications 2023 </font><br>
<a href="https://www.nature.com/articles/s41467-023-39520-3"><img src="data/paper.png"></a> 
<a href="https://github.com/bartulem/KISN-PyLab"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset"><img src="teasers/Sun2022.jpg"/></a></td>
<td align="left" width=550><em>MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior</em><br>
<font size=2.5>Jennifer J. Sun</font>, 
<font size=2.5>Markus Marks</font>, 
<font size=2.5>Andrew W. Ulmer</font>, 
<font size=2.5>Dipam Chakraborty</font>, 
<font size=2.5>Brian Geuther</font>, 
<font size=2.5>Edward Hayes</font>, 
<font size=2.5>Heng Jia</font>, 
<font size=2.5>Vivek Kumar</font>, 
<font size=2.5>Sebastian Oleszko</font>, 
<font size=2.5>Zachary Partridge</font>, 
<font size=2.5>Milan Peelman</font>, 
<font size=2.5>Alice Robie</font>, 
<font size=2.5>Catherine E. Schretter</font>, 
<font size=2.5>Keith Sheppard</font>, 
<font size=2.5>Chao Sun</font>, 
<font size=2.5>Param Uttarwar</font>, 
<font size=2.5>Julian M. Wagner</font>, 
<font size=2.5>Erik Werner</font>, 
<font size=2.5>Joseph Parker</font>, 
<font size=2.5>Pietro Perona</font>, 
<font size=2.5>Yisong Yue</font>, 
<font size=2.5>Kristin Branson</font>, 
<font size=2.5>Ann Kennedy</font><br>
<font size=2.5>In ICML 2023 </font><br>
<a href="https://openreview.net/pdf?id=Asrg2we3dP"><img src="data/paper.png"></a> 
<a href="https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/alexhang212/dataset-3dpop"><img src="teasers/Naik2023.jpg"/></a></td>
<td align="left" width=550><em>3D-POP - An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture</em><br>
<font size=2.5>Hemal Naik</font>, 
<font size=2.5>Alex Hoi Hang Chan</font>, 
<font size=2.5>Junran Yang</font>, 
<font size=2.5>Mathilde Delacoux</font>, 
<font size=2.5>Iain D. Couzin</font>, 
<font size=2.5>Fumihiro Kano</font>, 
<font size=2.5>Mate Nagy</font><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2303.13174.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/alexhang212/dataset-3dpop"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ye2023.jpg"/></a></td>
<td align="left" width=550><em>SuperAnimal models pretrained for plug-and-play analysis of animal behavior</em><br>
<a href="https://yeshaokai.github.io/"><font size=2.5>Shaokai Ye</font></a>, 
<font size=2.5>Anastasiia Filippova</font>, 
<font size=2.5>Jessy Lauer</font>, 
<font size=2.5>Maxime Vidal</font>, 
<font size=2.5>Steffen Schneider</font>, 
<font size=2.5>Tian Qiu</font>, 
<font size=2.5>Alexander Mathis</font>, 
<font size=2.5>Mackenzie Weygandt Mathis</font><br>
<font size=2.5>In arxiv 2023 </font><br>
<a href="https://arxiv-export3.library.cornell.edu/abs/2203.07436v2"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://www.tech4animals.org/catflw"><img src="teasers/Martvel2023.jpg"/></a></td>
<td align="left" width=550><em>CatFLW: Cat Facial Landmarks in the Wild Dataset</em><br>
<font size=2.5>George Martvel</font>, 
<font size=2.5>Nareed Farhat</font>, 
<font size=2.5>Ilan Shimshoni</font>, 
<font size=2.5>Anna Zamansky</font><br>
<font size=2.5>In CV4Animals Workshop in CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2305.04232.pdf"><img src="data/paper.png"></a> 
<a href="https://www.tech4animals.org/catflw"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a><a href="https://img.shields.io/badge/animal-cat-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-cat-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Li2023-2.jpg"/></a></td>
<td align="left" width=550><em>Decompose to Generalize: Species-Generalized Animal Pose Estimation</em><br>
<font size=2.5>Guangrui Li</font>, 
<font size=2.5>Yifan Sun</font>, 
<font size=2.5>Zongxin Yang</font>, 
<font size=2.5>Yi Yang</font><br>
<font size=2.5>In ICLR 2023 </font><br>
<a href="https://openreview.net/pdf?id=nQai_B1Zrt"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://mammal-net.github.io/#:~:text=We%20propose%20MammalNet%2C%20a%20large-scale%20video%20benchmark%20for,12%20common%20high-level%20mammal%20behaviors%20%28e.g.%2C%20hunt%2C%20groom%29."><img src="teasers/Chen2023.jpg"/></a></td>
<td align="left" width=550><em>MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding</em><br>
<a href="https://junchen14.github.io/"><font size=2.5>Jun Chen</font></a>, 
<a href="https://minghu0830.github.io/"><font size=2.5>Ming Hu</font></a>, 
<font size=2.5>Darren J. Coker</font>, 
<font size=2.5>Michael L. Berumen</font>, 
<font size=2.5>Blair Costelloe</font>, 
<font size=2.5>Sara Beery</font>, 
<a href="https://anna-rohrbach.net/"><font size=2.5>Anna Rohrbach</font></a>, 
<a href="https://www.mohamed-elhoseiny.com/research-work-and-publications.html"><font size=2.5>Mohamed Elhoseiny</font></a><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/abs/2306.00576"><img src="data/paper.png"></a> 
<a href="https://mammal-net.github.io/#:~:text=We%20propose%20MammalNet%2C%20a%20large-scale%20video%20benchmark%20for,12%20common%20high-level%20mammal%20behaviors%20%28e.g.%2C%20hunt%2C%20groom%29."><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/dattalab/keypoint-moseq"><img src="teasers/Weinreb2023.jpg"/></a></td>
<td align="left" width=550><em>Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics</em><br>
<font size=2.5>Caleb Weinreb</font>, 
<font size=2.5>Mohammed Abdal Monium Osman</font>, 
<font size=2.5>Libby Zhang</font>, 
<font size=2.5>Sherry Lin</font>, 
<font size=2.5>Jonah Pearl</font>, 
<font size=2.5>Sidharth Annapragada</font>, 
<font size=2.5>Eli Conlin</font>, 
<font size=2.5>Winthrop F. Gillis</font>, 
<font size=2.5>Maya Jay</font>, 
<font size=2.5>Ye Shaokai</font>, 
<font size=2.5>Alexander Mathis</font>, 
<font size=2.5>Mackenzie Weygandt Mathis</font>, 
<font size=2.5>Talmo Pereira</font>, 
<font size=2.5>Scott W. Linderman</font>, 
<font size=2.5>Sandeep Robert Datta</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.03.16.532307v2.full.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/dattalab/keypoint-moseq"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://bite.is.tue.mpg.de/"><img src="teasers/Ruegg2023.jpg"/></a></td>
<td align="left" width=550><em>BITE: Beyond Priors for Improved Three-D Dog Pose Estimation</em><br>
<font size=2.5>Nadine Rüegg</font>, 
<font size=2.5>Shashank Tripathi</font>, 
<font size=2.5>Konrad Schindler</font>, 
<font size=2.5>Michael J. Black</font>, 
<font size=2.5>Silvia Zuffi</font><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="none"><img src="data/paper.png"></a> 
<a href="https://bite.is.tue.mpg.de/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://3dmagicpony.github.io/"><img src="teasers/Wu2022.jpg"/></a></td>
<td align="left" width=550><em>MagicPony: Learning Articulated 3D Animals in the Wild</em><br>
<a href="https://elliottwu.com/"><font size=2.5>Shangzhe Wu</font></a>, 
<a href="https://ruiningli.com/"><font size=2.5>Ruining Li</font></a>, 
<a href="https://www.robots.ox.ac.uk/~tomj/"><font size=2.5>Tomas Jakab</font></a>, 
<a href="https://chrirupp.github.io/"><font size=2.5>Christian Rupprecht</font></a>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2211.12497.pdf"><img src="data/paper.png"></a> 
<a href="https://3dmagicpony.github.io/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Zhang2023.jpg"/></a></td>
<td align="left" width=550><em>CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose</em><br>
<font size=2.5>Xu Zhang</font>, 
<font size=2.5>Wen Wang</font>, 
<font size=2.5>Zhe Chen</font>, 
<font size=2.5>Yufei Xu</font>, 
<font size=2.5>Jing Zhang</font>, 
<font size=2.5>Dacheng Tao</font><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2206.11752.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ilett2023.jpg"/></a></td>
<td align="left" width=550><em>3D shape reconstruction of semi-transparent worms</em><br>
<font size=2.5>Thomas P. Ilett</font>, 
<font size=2.5>Omer Yuval</font>, 
<font size=2.5>Thomas Ranner</font>, 
<a href="https://eps.leeds.ac.uk/computing/staff/301/professor-netta-cohen"><font size=2.5>Netta Cohen</font></a>, 
<a href="https://eps.leeds.ac.uk/computing/staff/84/professor-david-hogg"><font size=2.5>David C. Hogg</font></a><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/abs/2304.14841"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Dutta2023.jpg"/></a></td>
<td align="left" width=550><em>A robust and flexible deep-learning workflow for animal tracking</em><br>
<a href="https://www.robots.ox.ac.uk/~adutta/"><font size=2.5>Abhishek Dutta</font></a>, 
<a href="https://www.biology.ox.ac.uk/people/natalia-perez-campanero-antolin"><font size=2.5>Natalia Pérez-Campanero</font></a>, 
<a href="https://www.biology.ox.ac.uk/people/professor-graham-taylor"><font size=2.5>Graham K. Taylor</font></a>, 
<a href="https://eng.ox.ac.uk/people/andrew-zisserman/"><font size=2.5>Andrew Zisserman</font></a>, 
<a href="https://oxnav.web.ox.ac.uk/people/cait-newport"><font size=2.5>Cait Newport</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.04.20.537633v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kohno2023.jpg"/></a></td>
<td align="left" width=550><em>Analysis of antennal responses to motion stimuli in the honey bee by automated tracking using DeepLabCut</em><br>
<font size=2.5>Hiroki Kohno</font>, 
<font size=2.5>Shuichi Kamata</font>, 
<font size=2.5>Takeo Kubo</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.04.24.538069v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://statho.github.io/projects/animals3d/index.html"><img src="teasers/Stathopoulos2023.jpg"/></a></td>
<td align="left" width=550><em>Learning Articulated Shape with Keypoint Pseudo-labels from Web Images</em><br>
<a href="https://statho.github.io/"><font size=2.5>Anastasis Stathopoulos</font></a>, 
<a href="https://geopavlakos.github.io/"><font size=2.5>Georgios Pavlakos</font></a>, 
<a href="https://phymhan.github.io/"><font size=2.5>Ligong Han</font></a>, 
<a href="https://www.cs.rutgers.edu/~dnm/"><font size=2.5>Dimitris Metaxas</font></a><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2304.14396.pdf"><img src="data/paper.png"></a> 
<a href="https://statho.github.io/projects/animals3d/index.html"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://farm3d.github.io/"><img src="teasers/Jakab2023.jpg"/></a></td>
<td align="left" width=550><em>Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion</em><br>
<a href="https://www.robots.ox.ac.uk/~tomj/"><font size=2.5>Tomas Jakab</font></a>, 
<a href="https://ruiningli.com/"><font size=2.5>Ruining Li</font></a>, 
<a href="https://elliottwu.com/"><font size=2.5>Shangzhe Wu</font></a>, 
<a href="https://chrirupp.github.io/"><font size=2.5>Christian Rupprecht</font></a>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a><br>
<font size=2.5>In arxiv 2023 </font><br>
<a href="https://arxiv.org/pdf/2304.10535.pdf"><img src="data/paper.png"></a> 
<a href="https://farm3d.github.io/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://gigazoom.rc.duke.edu/"><img src="teasers/Zhou2023.jpg"/></a></td>
<td align="left" width=550><em>Parallelized computational 3D video microscopy of freely moving organisms at multiple gigapixels per second</em><br>
<font size=2.5>Kevin C. Zhou</font>, 
<font size=2.5>Mark Harfouche</font>, 
<font size=2.5>Colin L. Cooke</font>, 
<font size=2.5>Jaehee Park</font>, 
<font size=2.5>Pavan C. Konda</font>, 
<font size=2.5>Lucas Kreiss</font>, 
<font size=2.5>Kanghyun Kim</font>, 
<font size=2.5>Joakim Jönsson</font>, 
<font size=2.5>Thomas Doman</font>, 
<font size=2.5>Paul Reamey</font>, 
<font size=2.5>Veton Saliu</font>, 
<font size=2.5>Clare B. Cook</font>, 
<font size=2.5>Maxwell Zheng</font>, 
<font size=2.5>John P. Bechtel</font>, 
<font size=2.5>Aurélien Bègue</font>, 
<font size=2.5>Matthew McCarroll</font>, 
<font size=2.5>Jennifer Bagwell</font>, 
<font size=2.5>Gregor Horstmeyer</font>, 
<font size=2.5>Michel Bagnat</font>, 
<font size=2.5>Roarke Horstmeyer</font><br>
<font size=2.5>In Nature Photonics 2023 </font><br>
<a href="https://www.nature.com/articles/s41566-023-01171-7"><img src="data/paper.png"></a> 
<a href="https://gigazoom.rc.duke.edu/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Zhu2023.jpg"/></a></td>
<td align="left" width=550><em>Scalable Apparatus to Measure Posture and Locomotion (SAMPL): a high-throughput solution to study unconstrained vertical behavior in small animals</em><br>
<font size=2.5>Yunlu Zhu</font>, 
<font size=2.5>Franziska Auer</font>, 
<font size=2.5>Hannah Gelnaw</font>, 
<font size=2.5>Samantha N. Davis</font>, 
<font size=2.5>Kyla R. Hamling</font>, 
<font size=2.5>Christina E. May</font>, 
<font size=2.5>Hassan Ahamed</font>, 
<font size=2.5>Niels Ringstad</font>, 
<font size=2.5>Katherine I. Nagel</font>, 
<font size=2.5>David Schoppik</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.01.07.523102v2.full.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Li2023.jpg"/></a></td>
<td align="left" width=550><em>ScarceNet: Animal Pose Estimation with Scarce Annotations</em><br>
<font size=2.5>Chen Li</font>, 
<a href="https://www.comp.nus.edu.sg/~leegh/"><font size=2.5>Gim Hee Lee</font></a><br>
<font size=2.5>In CVPR 2023 </font><br>
<a href="https://arxiv.org/pdf/2303.15023.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/Asad127/3D-RECONSTRUCTION-USING-SINGLE-CAMERA-AND-MIRROR-SETUP"><img src="teasers/Hussain2023.jpg"/></a></td>
<td align="left" width=550><em>Lights, Camera, Mirrors, Action! Toolbox for 3D Analysis of High-rate Maneuvers Using a Single Camera and Planar Mirrors</em><br>
<font size=2.5>Wajahat Hussain</font>, 
<font size=2.5>Mahum Naveed</font>, 
<font size=2.5>Asad Khan</font>, 
<font size=2.5>Taimoor Hasan Khan</font>, 
<font size=2.5>Muhammad Latif Anjum</font>, 
<font size=2.5>Shahzad Rasool</font>, 
<font size=2.5>Adnan Maqsood</font><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.03.14.532636v1"><img src="data/paper.png"></a> 
<a href="https://github.com/Asad127/3D-RECONSTRUCTION-USING-SINGLE-CAMERA-AND-MIRROR-SETUP"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Mimura2023.jpg"/></a></td>
<td align="left" width=550><em>Unsupervised decomposition of natural monkey behavior into a sequence of motion motifs</em><br>
<font size=2.5>Koki Mimura</font>, 
<font size=2.5>Jumpei Matsumoto</font>, 
<font size=2.5>Daichi Mochihashi</font>, 
<font size=2.5>Tomoaki Nakamura</font>, 
<font size=2.5>Toshiyuki Hirabayashi</font>, 
<font size=2.5>Makoto Higuchi</font>, 
<a href="https://researchmap.jp/minamoto"><font size=2.5>Takafumi Minamimoto</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.03.04.531044v3.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Bohnslav2023.jpg"/></a></td>
<td align="left" width=550><em>ArMo: An Articulated Mesh Approach for Mouse 3D Reconstruction</em><br>
<font size=2.5>James P. Bohnslav</font>, 
<font size=2.5>Mohammed Abdal Monium Osman</font>, 
<font size=2.5>Akshay Jaggi</font>, 
<font size=2.5>Sofia Soares</font>, 
<font size=2.5>Caleb Weinreb</font>, 
<a href="http://datta.hms.harvard.edu/"><font size=2.5>Sandeep Robert Datta</font></a>, 
<a href="https://harveylab.hms.harvard.edu/"><font size=2.5>Christopher D. Harvey</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.02.17.526719v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Papaspyros2023.jpg"/></a></td>
<td align="left" width=550><em>Predicting long-term collective animal behavior with deep learning</em><br>
<font size=2.5>Vaios Papaspyros</font>, 
<font size=2.5>Ram´on Escobedo</font>, 
<font size=2.5>Alexandre Alahi</font>, 
<font size=2.5>Guy Theraulaz</font>, 
<font size=2.5>Cl´ement Sire</font>, 
<a href="https://people.epfl.ch/francesco.mondada?lang=en"><font size=2.5>Francesco Mondada</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.02.15.528318v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/Aghileh/UPPER"><img src="teasers/Ebrahimi2023.jpg"/></a></td>
<td align="left" width=550><em>Three‑dimensional unsupervised probabilistic pose reconstruction (3D‑UPPER) for freely moving animals</em><br>
<font size=2.5>Aghileh S. Ebrahimi</font>, 
<font size=2.5>Patrycja Orlowska‑Feuer</font>, 
<font size=2.5>Qian Huang</font>, 
<font size=2.5>AntonioG. Zippo</font>, 
<font size=2.5>Franck P. Martial</font>, 
<font size=2.5>Rasmus S. Petersen</font>, 
<a href="https://research.manchester.ac.uk/en/persons/riccardo.storchi"><font size=2.5>Riccardo Storchi</font></a><br>
<font size=2.5>In Scientific Reports 2023 </font><br>
<a href="https://www.nature.com/articles/s41598-022-25087-4"><img src="data/paper.png"></a> 
<a href="https://github.com/Aghileh/UPPER"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Panadeiro2021.jpg"/></a></td>
<td align="left" width=550><em>A review of 28 free animal-tracking software applications: current features and limitations</em><br>
<font size=2.5>Veronica Panadeiro</font>, 
<font size=2.5>Alvaro Rodriguez</font>, 
<font size=2.5>Jason Henry</font>, 
<font size=2.5>Donald Wlodkowic</font>, 
<font size=2.5>Magnus Andersson</font><br>
<font size=2.5>In Lab Animal 2021 </font><br>
<a href="https://www.nature.com/articles/s41684-021-00811-1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/KumarLabJax/JABS-data-pipeline/tree/main"><img src="teasers/Beane2023.jpg"/></a></td>
<td align="left" width=550><em>JAX Animal Behavior System (JABS): A video-based phenotyping platform for the laboratory mouse</em><br>
<font size=2.5>Glen Beane</font>, 
<font size=2.5>Brian Q. Geuther</font>, 
<font size=2.5>Thomas J. Sproule</font>, 
<font size=2.5>Anshul Choudhary</font>, 
<font size=2.5>Jarek Trapszo</font>, 
<font size=2.5>Leinani Hession</font>, 
<font size=2.5>Vivek Kohar</font>, 
<a href="https://www.jax.org/research-and-faculty/faculty/vivek-kumar"><font size=2.5>Vivek Kumar</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.01.13.476229v2.full.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/KumarLabJax/JABS-data-pipeline/tree/main"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Hernandez2023.jpg"/></a></td>
<td align="left" width=550><em>An 8-cage imaging system for automated analyses of mouse behavior</em><br>
<font size=2.5>Thaís Del Rosario Hernández</font>, 
<font size=2.5>Narendra R. Joshi</font>, 
<font size=2.5>Sayali V. Gore</font>, 
<font size=2.5>Jill A. Kreiling</font>, 
<a href="https://vivo.brown.edu/display/rcretonp"><font size=2.5>Robbert Creton</font></a><br>
<font size=2.5>In biorxiv 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2023.02.04.527129v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/benkoger/overhead-video-worked-examples"><img src="teasers/Koger2022.jpg"/></a></td>
<td align="left" width=550><em>Quantifying the movement, behavior, and environmental context of group-living animals using drones and computer vision</em><br>
<font size=2.5>Benjamin Koger</font>, 
<font size=2.5>Adwait Deshpande</font>, 
<font size=2.5>Jeffrey T. Kerby</font>, 
<font size=2.5>Jacob M. Graving</font>, 
<font size=2.5>Blair R. Costelloe</font>, 
<a href="https://collectivebehaviour.com/people/couzin-iain/"><font size=2.5>Iain D. Couzin</font></a><br>
<font size=2.5>In Journal of Animal Ecology 2023 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.06.30.498251v2.full.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/benkoger/overhead-video-worked-examples"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Sheppard2020.jpg"/></a></td>
<td align="left" width=550><em>Gait-level analysis of mouse open field behavior using deep learning-based pose estimation</em><br>
<font size=2.5>Keith Sheppard</font>, 
<font size=2.5>Justin Gardin</font>, 
<font size=2.5>Gautam Sabnis</font>, 
<font size=2.5>Asaf Peer</font>, 
<font size=2.5>Megan Darrell</font>, 
<font size=2.5>Sean Deats</font>, 
<font size=2.5>Brian Geuther</font>, 
<font size=2.5>Cathleen M. Lutz</font>, 
<a href="https://www.kumarlab.org/"><font size=2.5>Vivek Kumar</font></a><br>
<font size=2.5>In Cell Reports 2022 </font><br>
<a href="https://www.cell.com/action/showPdf?pii=S2211-1247%2821%2901740-X"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Bozek2021.jpg"/></a></td>
<td align="left" width=550><em>Markerless tracking of an entire honey bee colony</em><br>
<font size=2.5>Katarzyna Bozek</font>, 
<font size=2.5>Laetitia Hebert</font>, 
<font size=2.5>Yoann Portugal</font>, 
<font size=2.5>Alexander S. Mikheyev</font>, 
<a href="https://research.vu.nl/en/persons/gj-stephens/publications/"><font size=2.5>Greg J. Stephens</font></a><br>
<font size=2.5>In Nature Communications 2021 </font><br>
<a href="https://www.nature.com/articles/s41467-021-21769-1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ahamed2020.jpg"/></a></td>
<td align="left" width=550><em>Capturing the continuous complexity of behaviour in Caenorhabditis elegans</em><br>
<font size=2.5>Tosif Ahamed</font>, 
<font size=2.5>Antonio C. Costa</font>, 
<a href="https://research.vu.nl/en/persons/gj-stephens/publications/"><font size=2.5>Greg J. Stephens</font></a><br>
<font size=2.5>In Nature Physics 2020 </font><br>
<a href="https://www.nature.com/articles/s41567-020-01036-8"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Zhou2022-1.jpg"/></a></td>
<td align="left" width=550><em>ConstrastivePose: A contrastive learning approach for self-supervised feature engineering for pose estimation and behavorial classification of interacting animals</em><br>
<font size=2.5>Tianxun Zhou</font>, 
<font size=2.5>Calvin Chee Hoe Cheah</font>, 
<font size=2.5>Eunice Wei Mun Chin</font>, 
<font size=2.5>Jie Chen</font>, 
<font size=2.5>Hui Jia Farm</font>, 
<font size=2.5>Eyleen Lay Keow Goh</font>, 
<font size=2.5>Keng Hwee Chiam</font><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.11.09.515746v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Sinha2022.jpg"/></a></td>
<td align="left" width=550><em>Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories</em><br>
<a href="https://www.samsinha.me/"><font size=2.5>Samarth Sinha</font></a>, 
<font size=2.5>Roman Shapovalov</font>, 
<font size=2.5>Jeremy Reizenstein</font>, 
<font size=2.5>Ignacio Rocco</font>, 
<a href="https://nneverova.github.io/"><font size=2.5>Natalia Neverova</font></a>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/pub.html"><font size=2.5>Andrea Vedaldi</font></a>, 
<a href="https://d-novotny.github.io/"><font size=2.5>David Novotny</font></a><br>
<font size=2.5>In arxiv 2022 </font><br>
<a href="https://arxiv.org/abs/2211.03889v1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-cat-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-cat-yellowgreen"></a><a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://bsoid.org/"><img src="teasers/Hsu2022.jpg"/></a></td>
<td align="left" width=550><em>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</em><br>
<font size=2.5>Alexander I. Hsu</font>, 
<a href="https://www.cmu.edu/bio/people/faculty/yttri.html"><font size=2.5>Eric A. Yttri</font></a><br>
<font size=2.5>In Nature Communications 2022 </font><br>
<a href="https://www.nature.com/articles/s41467-021-25420-x"><img src="data/paper.png"></a> 
<a href="https://bsoid.org/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/bbo-lab/ACM"><img src="teasers/Monsees2022.jpg"/></a></td>
<td align="left" width=550><em>Estimation of skeletal kinematics in freely moving rodents</em><br>
<font size=2.5>Arne Monsees</font>, 
<font size=2.5>Kay-Michael Voit</font>, 
<font size=2.5>Damian J. Wallace</font>, 
<font size=2.5>Juergen Sawinski</font>, 
<font size=2.5>Edyta Charyasz</font>, 
<font size=2.5>Klaus Scheffler</font>, 
<font size=2.5>Jakob H. Macke</font>, 
<font size=2.5>Jason N. D. Kerr</font><br>
<font size=2.5>In Nature Methods 2022 </font><br>
<a href="https://www.nature.com/articles/s41592-022-01634-9"><img src="data/paper.png"></a> 
<a href="https://github.com/bbo-lab/ACM"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/shlizee/OpenLabCluster"><img src="teasers/Li2022-2.jpg"/></a></td>
<td align="left" width=550><em>OpenLabCluster: Active Learning Based Clustering and Classification of Animal Behaviors in Videos Based on Automatically Extracted Kinematic Body Keypoints</em><br>
<font size=2.5>Jingyuan Li</font>, 
<font size=2.5>Moishe Keselman</font>, 
<font size=2.5>Eli Shlizerman</font><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.10.10.511660v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/shlizee/OpenLabCluster"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://chhankyao.github.io/lassie/"><img src="teasers/Yao2022.jpg"/></a></td>
<td align="left" width=550><em>LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery</em><br>
<a href="https://www.chhankyao.com/"><font size=2.5>Chun-Han Yao</font></a>, 
<font size=2.5>Wei-Chih Hung</font>, 
<font size=2.5>Yuanzhen Li</font>, 
<font size=2.5>Michael Rubinstein</font>, 
<a href="http://faculty.ucmerced.edu/mhyang/"><font size=2.5>Ming-Hsuan Yang</font></a>, 
<a href="https://varunjampani.github.io/"><font size=2.5>Varun Jampani</font></a><br>
<font size=2.5>In NeurIPS 2022 </font><br>
<a href="https://arxiv.org/pdf/2207.03434.pdf"><img src="data/paper.png"></a> 
<a href="https://chhankyao.github.io/lassie/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kim2022-2.jpg"/></a></td>
<td align="left" width=550><em>Evaluation of mouse behavioral responses to nutritive versus nonnutritive sugar using a deep learning-based 3D real-time pose estimation system</em><br>
<font size=2.5>Jineun Kim</font>, 
<font size=2.5>Dae-gun Kim</font>, 
<font size=2.5>Wongyo Jung</font>, 
<a href="https://med.nyu.edu/faculty/seong-bae-greg-suh"><font size=2.5>Greg S. B. Suh</font></a><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.09.19.508605v1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Yurimoto2022.jpg"/></a></td>
<td align="left" width=550><em>Development of a new 3D tracking system for multiple marmosets under free-moving conditions</em><br>
<font size=2.5>Terumi Yurimoto</font>, 
<font size=2.5>Wakako Kumita</font>, 
<font size=2.5>Kenya Sato</font>, 
<font size=2.5>Rika Kikuchi</font>, 
<font size=2.5>Yusuke Shibuki</font>, 
<font size=2.5>Rino Hashimoto</font>, 
<font size=2.5>Michiko Kamioka</font>, 
<font size=2.5>Yumi Hayasegawa</font>, 
<font size=2.5>Eiko Yamazaki</font>, 
<font size=2.5>Yoko Kurotaki</font>, 
<font size=2.5>Norio Goda</font>, 
<font size=2.5>Junichi Kitakami</font>, 
<font size=2.5>Tatsuya Fujita</font>, 
<font size=2.5>Takashi Inoue</font>, 
<font size=2.5>Erika Sasaki</font><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.03.29.486138v3.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/umyelab/LabGym"><img src="teasers/Hu2022.jpg"/></a></td>
<td align="left" width=550><em>LabGym: quantification of user-defined animal behaviors using learning-based holistic assessment</em><br>
<font size=2.5>Yujia Hu</font>, 
<font size=2.5>Carrie R. Ferrario</font>, 
<font size=2.5>Alexander D. Maitland</font>, 
<font size=2.5>Rita B. Ionides</font>, 
<font size=2.5>Anjesh Ghimire</font>, 
<font size=2.5>Brendon Watson</font>, 
<font size=2.5>Kenichi Iwasaki</font>, 
<font size=2.5>Hope White</font>, 
<font size=2.5>Yitao Xi</font>, 
<font size=2.5>Jie Zhou</font>, 
<a href="https://www.lsi.umich.edu/science/our-labs/bing-ye-lab"><font size=2.5>Bing Ye</font></a><br>
<font size=2.5>In Cell Reports Methods 2022 </font><br>
<a href="https://www.cell.com/cell-reports-methods/fulltext/S2667-2375(23)00026-7"><img src="data/paper.png"></a> 
<a href="https://github.com/umyelab/LabGym"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/DeNardoLab/BehaviorDEPOT"><img src="teasers/Gabriel2022.jpg"/></a></td>
<td align="left" width=550><em>BehaviorDEPOT is a simple, flexible tool for automated behavioral detection based on markerless pose tracking</em><br>
<font size=2.5>Christopher J Gabriel</font>, 
<font size=2.5>Zachary Zeidler</font>, 
<font size=2.5>Benita Jin</font>, 
<font size=2.5>Changliang Guo</font>, 
<font size=2.5>Caitlin M Goodpaster</font>, 
<font size=2.5>Adrienne Q Kashay</font>, 
<font size=2.5>Anna Wu</font>, 
<font size=2.5>Molly Delaney</font>, 
<font size=2.5>Jovian Cheung</font>, 
<font size=2.5>Lauren E DiFazio</font>, 
<font size=2.5>Melissa J Sharpe</font>, 
<font size=2.5>Daniel Aharoni</font>, 
<font size=2.5>Scott A Wilke</font>, 
<a href="https://wd2labs.org/denardo-lab"><font size=2.5>Laura A DeNardo</font></a><br>
<font size=2.5>In eLife 2022 </font><br>
<a href="https://elifesciences.org/articles/74314"><img src="data/paper.png"></a> 
<a href="https://github.com/DeNardoLab/BehaviorDEPOT"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Zhou2022.jpg"/></a></td>
<td align="left" width=550><em>Cross-Skeleton Interaction Graph Aggregation Network for Representation Learning of Mouse Social Behaviour</em><br>
<font size=2.5>Feixiang Zhou</font>, 
<font size=2.5>Xinyu Yang</font>, 
<font size=2.5>Fang Chen</font>, 
<font size=2.5>Long Chen</font>, 
<font size=2.5>Zheheng Jiang</font>, 
<font size=2.5>Hui Zhu</font>, 
<font size=2.5>Reiko Heckel</font>, 
<font size=2.5>Haikuan Wang</font>, 
<font size=2.5>Minrui Fei</font>, 
<a href="https://bipl-uol.github.io/"><font size=2.5>Huiyu Zhou</font></a><br>
<font size=2.5>In arxiv 2022 </font><br>
<a href="https://arxiv.org/pdf/2208.03819.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Jiang2022.jpg"/></a></td>
<td align="left" width=550><em>Animal pose estimation: A closer look at the state-of-the-art, existing gaps and opportunities</em><br>
<font size=2.5>Le Jiang</font>, 
<font size=2.5>Caleb Lee</font>, 
<font size=2.5>Divyang Teotia</font>, 
<a href="https://coe.northeastern.edu/people/ostadabbas-sarah/"><font size=2.5>Sarah Ostadabbas</font></a><br>
<font size=2.5>In Computer Vision and Image Understanding (CVIU) 2022 </font><br>
<a href="https://pdf.sciencedirectassets.com/271018/1-s2.0-S1077314222X00088/1-s2.0-S1077314222000893/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIFQBZBGc%2FIJ6aPoZ3UeHo3HM3Dc8FiVDb2HvRztSbMArAiEAkT%2B0QjP9moH8oJRMGp2y10SQoytVcsd3xGYscSL4xcgq0gQIFRAFGgwwNTkwMDM1NDY4NjUiDFEDG8p47QDsIUlG%2ByqvBA5JI8kfEowgxt2IgXPGTC9E0poRYc%2F17MaMbzPlftJgLE8vG1BmYC00Kxani1fUvOZK%2F20F4GjixcF3uG3cxAKjINCPMmQcLIBZb1P%2BW8VFxX1nKi5PYwxu09t4Lvh41kGsKK4V2IV5%2FO3PWHNnP0je%2FZQZTMQJOztxbdOjs4yzaWOfeoq9r07%2BbHSUJnZXjUGc%2BFNQtjf7WQthAUHyWCeLA6wNC5SAPnwQBUf1YP%2Bp9Xcaql1cPZIx3BWJg5eFgvVwLAGZD8ozUSzb4tBkXgFBD2gyLL304xiGnrAMo1IC7RS9CGpcX7EoJh%2FOwFwQJhYlcfZiqhlHCv61C99HWsRpI0aVXGTpMM14LK5RIJjO%2BcrRiNPuiwZOwC9srBqKvzYlLyp19%2FkRb4yxSFXQc0mhv9JP4%2Fqp19TCN8FcheS4hQD9AfoS3VpgyN0LiN8xwN1Z00Q4E2%2FQXr5mV21UALyJEv2TY2gA%2BUwsFdfO4%2FCPLhf61liCvvxA%2BA2Gh60QT5qIvyxy8SXSRmCsl8nTx%2FYSsVyqhnWuOCgjUgma6nBv27J9pAb4FUVCvPHuI56CJqMrs3eTVwzvTaPNxkLMahif%2B%2Fe8NLQ00D89DTCyR0PVqhwXluDHcTdMkrPlj9h3B577RFyYuHtKfaLC8GF0KudmQzVUb2sDJ9wTs0%2FJf%2BvAW90hcuE7yYuhLAQ6wPqnzVnHynPUM2R02FkYbOkGXKy8pgRErzWM%2Fqv%2BijtcNBwwmu%2FDlwY6qQG%2Fg9SPZlh7cee4Gbjbf%2F6DQ%2BeomRX8uLKle0wketa9AqGuVIxRm4pnhdpXlbwFf1sOWaOA2uxPDLMjyBLkC0cFk9jLAtl5%2F4Ce5d24dp4CVhwL%2FlnLTAnZZ6QZ6ODJuiTdViFnhak5EXdfewd8d1VAXYlxXSuuAo36P3imWvdE6ZZRprP5tHdUsK3FWccb3n9y4DLALnmpvNJZK9l38klSYdB0i4QY6sZY&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220808T122023Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5ENN2VZW%2F20220808%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=7e952d9ffae1fad55c2f335e51a4bf640b38945991840d406cc6b3d6632c6020&hash=bc8710f2757341861668832706bd93e498378167e21428b1330e5275d61add91&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1077314222000893&tid=spdf-8474dc97-a5ce-4761-90d5-dcf16f473e47&sid=3a219f905ca9224d262a48d78c6e495482c7gxrqa&type=client&ua=4d5753565d5303025601&rr=737834ac4c017d6b"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/lmb-freiburg/FreiPose-docker"><img src="teasers/Schneider2022.jpg"/></a></td>
<td align="left" width=550><em>3D pose estimation enables virtual head fixation in freely moving rats</em><br>
<font size=2.5>Artur Schneider</font>, 
<font size=2.5>Christian Zimmermann</font>, 
<font size=2.5>Mansour Alyahyay</font>, 
<font size=2.5>Florian Steenbergen</font>, 
<a href="https://lmb.informatik.uni-freiburg.de/people/brox/index.en.html"><font size=2.5>Thomas Brox</font></a>, 
<a href="https://www.optophysiology.uni-freiburg.de/team/Ilka-Diester/"><font size=2.5>Ilka Diester</font></a><br>
<font size=2.5>In Neuron 2022 </font><br>
<a href="https://pdf.sciencedirectassets.com/272195/1-s2.0-S0896627321X00141/1-s2.0-S0896627322003646/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDV6N%2F%2BBt%2BGqIlLDrkHchQZhdD2nBYt%2FFrG3S%2Fmba2VPAIhAPRNhI8h5MnDqdLO1WwhVHcocD2L71SpclrTT3pfyxDkKtIECCQQBRoMMDU5MDAzNTQ2ODY1IgxCNvxZZO%2FMJWMBhKcqrwRfTmBNeCQMrKgBl1IJa7ocCCqAUGeDpRXd5dHbZluslCw7zvTNs9CEmWI7R%2F5ccG7j7Q6%2Fhfm2HE3hWq2MT4GCCnO7lLBosKDaz%2FDPQXxxeX77Sm2RQEX5yICZfpIuZXI4W0Yho8rVqOQcomZMgqSAAQgYp313sR%2F2%2FxlABX4MCg9DU1D5F%2BFt8le7BJrxEZaPkfx6BohT8PbJqwSY%2F1hX9mDSRelp8di3J7igt%2BNpTGHLKA0M7M8t4H6CZGv6xSrMtyf6fTGS33C7tmsKTwHO%2Ba%2BoLTkGsQpSO%2FscjKLLolbHy3jBAsQipmggnJkx4IpcSMWL2d5iWjhI5XWTq7gUZLnq3SHXlHR%2BuMW03tI3ucaAMVrhCNx2HE2ZJfWo9MMxQGacCIoVp3lzIiCLig%2FMMeTl15EyGPifO1MNljJfH5cdlhZ5r0DOvOVrgBDd9JXEUn3zgLp45%2BwKDrh9CzgCsoLPxD%2B%2BLMZwDK%2F9dE7bAJKaCgo85T%2FhoNy7A%2FhQLIHjKcu81Zqt5D2D9wy8rDQHz13x%2FCbD76ajAifk%2F6bFonMEfN6ZXYSTWbJKgeQGBIPsZErkznK8R907eUpTOzr270tTXRym9XEBM1Lxg6Jbbj%2BsfwSxqkr0DhXcb%2B9IgWH74UYDUY1gjeQF6VtRbv3qCSM1kzwJwShfE%2FFeP375Fxlg9jSL58VNoWTuD3YHEZfEjOtlWFfOufgq4emnkdGlJj4bU7zdjo7%2FE1Z%2FCFcgMMPl3ZYGOqgBdAWyyK0nFa25asgZlTt6BK1N60AHIOmfG6X77ucOP%2FPluSR7QHFxjvz23mKgOLw4QMAY63GVRN9FUoh4zvie5xILoLiJDDPo6NyZtqvv3pR7hQnrBJTy08n0v7XeEdgDq6WktQ6ozyCDEhyGUh4IRyX73SEOGDts1tUrlUCaBFVsgegVx2pRUU5ZCHRmBOcOtc5b1n5R5zT9448EprxwmDuDOc8XPh5H&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220720T044040Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY627OCQ44%2F20220720%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=66ee8723bc2e76b3e1ab91f6ab91db634bbb3c222930353d94bf47dd8d5aa039&hash=e99217d7352542037a12a9e798ccca9cf6835587ae84c2a81811c736d4c10f22&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0896627322003646&tid=spdf-05a97aff-ea73-484b-9355-aa1ee366f3c7&sid=493fed999bf8674a09191fd1bf0dafd657aegxrqa&type=client&ua=4d5600575e5250500107&rr=72d90521cefa34b1"><img src="data/paper.png"></a> 
<a href="https://github.com/lmb-freiburg/FreiPose-docker"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Miller2022.jpg"/></a></td>
<td align="left" width=550><em>Natural behavior is the language of the brain</em><br>
<font size=2.5>Cory T. Miller</font>, 
<font size=2.5>David Gire</font>, 
<font size=2.5>Kim Hoke</font>, 
<font size=2.5>Alexander C. Huk</font>, 
<font size=2.5>Darcy Kelley</font>, 
<font size=2.5>David A. Leopold</font>, 
<font size=2.5>Matthew C. Smear</font>, 
<font size=2.5>Frederic Theunissen</font>, 
<font size=2.5>Michael Yartsev</font>, 
<a href="https://nielllab.uoregon.edu/people/"><font size=2.5>Cristopher M. Niell</font></a><br>
<font size=2.5>In Current Biology 2022 </font><br>
<a href="https://www.sciencedirect.com/science/article/pii/S0960982222004262?via%3Dihub"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://edspace.american.edu/openbehavior/project/sipec/"><img src="teasers/Marks2022.jpg"/></a></td>
<td align="left" width=550><em>Deep-learning based identification, tracking, pose estimation, and behavior classification of interacting primates and mice in complex environments</em><br>
<font size=2.5>Markus Marks</font>, 
<font size=2.5>Jin Qiuhan</font>, 
<font size=2.5>Oliver Sturman</font>, 
<font size=2.5>Lukas von Ziegler</font>, 
<font size=2.5>Sepp Kollmorgen</font>, 
<font size=2.5>Wolfger von der Behrens</font>, 
<font size=2.5>Valerio Mante</font>, 
<font size=2.5>Johannes Bohacek</font>, 
<font size=2.5>Mehmet Fatih Yanik</font><br>
<font size=2.5>In Nature Machine Intelligence 2022 </font><br>
<a href="https://www.nature.com/articles/s42256-022-00477-5"><img src="data/paper.png"></a> 
<a href="https://edspace.american.edu/openbehavior/project/sipec/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ibanez2022.jpg"/></a></td>
<td align="left" width=550><em>EXPLORE: A novel deep learning-based analysis method for exploration behaviour in object recognition tests</em><br>
<font size=2.5>Victor Iba˜nez</font>, 
<font size=2.5>Laurens Bohlen</font>, 
<font size=2.5>Francesca Manuell</font>, 
<font size=2.5>Isabelle Mansuy</font>, 
<font size=2.5>Fritjof Helmchen</font>, 
<a href="https://www.hifo.uzh.ch/en/research/helmchen/juniorgroup0/publication.html"><font size=2.5>Anna-Sophia Wahl</font></a><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2022.06.24.497470v1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Walter2020.jpg"/></a></td>
<td align="left" width=550><em>TRex, a fast multi-animal tracking system with markerless identification, 2D body posture estimation and visual field reconstruction</em><br>
<a href="https://www.orn.mpg.de/person/45292/409958"><font size=2.5>Tristan Walter</font></a>, 
<a href="http://collectivebehaviour.com/"><font size=2.5>Iain D Couzin</font></a><br>
<font size=2.5>In eLife 2021 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2020.10.14.338996v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Brattoli2021.jpg"/></a></td>
<td align="left" width=550><em>Unsupervised behaviour analysis and magnification (uBAM) using deep learning</em><br>
<font size=2.5>Biagio Brattoli</font>, 
<font size=2.5>Uta Büchler</font>, 
<font size=2.5>Michael Dorkenwald</font>, 
<font size=2.5>Philipp Reiser</font>, 
<font size=2.5>Linard Filli</font>, 
<font size=2.5>Fritjof Helmchen</font>, 
<a href="https://www.hifo.uzh.ch/en/research/helmchen/juniorgroup0/publication.html"><font size=2.5>Anna-Sophia Wahl</font></a>, 
<font size=2.5>Björn Ommer</font><br>
<font size=2.5>In Nature Machine Intelligence 2021 </font><br>
<a href="https://www.nature.com/articles/s42256-021-00326-x"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Marshall2021.jpg"/></a></td>
<td align="left" width=550><em>Leaving Flatland: Advances in 3D behavioral measurement</em><br>
<a href="https://neurotree.org/neurotree/peopleinfo.php?pid=663448"><font size=2.5>Jesse D. Marshall</font></a>, 
<font size=2.5>Tianqing Li</font>, 
<font size=2.5>Joshua H. Wu</font>, 
<font size=2.5>Timothy W. Dunn</font><br>
<font size=2.5>In Current Opinion in Neurobiology 2022 </font><br>
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0959438822000071?via%3Dihub"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/tqxli/dannce-pytorch"><img src="teasers/Li2022-1.jpg"/></a></td>
<td align="left" width=550><em>Improved Markerless 3D Animal Pose Estimation Using Temporal Semi-Supervision</em><br>
<font size=2.5>Tianqing Li</font>, 
<font size=2.5>Kyle S. Severson</font>, 
<font size=2.5>Fan Wang</font>, 
<font size=2.5>Timothy W. Dunn</font><br>
<font size=2.5>In CV4Animal workshop at CVPR 2022 </font><br>
<a href="https://drive.google.com/file/d/1lZcrXlazErthSkPJtR6g1DLhFo1HmwXU/view?usp=sharing"><img src="data/paper.png"></a> 
<a href="https://github.com/tqxli/dannce-pytorch"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ramalingasetty2017.jpg"/></a></td>
<td align="left" width=550><em>A whole-body musculoskeletal model of the mouse</em><br>
<font size=2.5>Shravan Tata Ramalingasetty</font>, 
<font size=2.5>Simon M. Danner</font>, 
<font size=2.5>Jonathan Arreguit</font>, 
<font size=2.5>Sergey N. Markin</font>, 
<font size=2.5>Dimitri Rodarie</font>, 
<font size=2.5>Claudia Kathe</font>, 
<font size=2.5>Grégoire Courtine</font>, 
<font size=2.5>Ilya A. Rybak</font>, 
<a href="https://www.epfl.ch/labs/biorob/"><font size=2.5>Auke Ijspeert</font></a><br>
<font size=2.5>In IEEE Access 2021 </font><br>
<a href="https://ieeexplore.ieee.org/document/9638502/"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://demoavataranalysis.ap.ngrok.io/"><img src="teasers/Kim2022.jpg"/></a></td>
<td align="left" width=550><em>AVATAR: AI Vision Analysis for Three-dimensional Action in Real-time</em><br>
<font size=2.5>Dae-Gun Kim</font>, 
<font size=2.5>Anna Shin</font>, 
<font size=2.5>Yong-Cheol Jeong</font>, 
<font size=2.5>Seahyung Park</font>, 
<a href="https://www.researchgate.net/profile/Daesoo-Kim-2"><font size=2.5>Daesoo Kim</font></a><br>
<font size=2.5>In biorxiv 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2021.12.31.474634v1"><img src="data/paper.png"></a> 
<a href="https://demoavataranalysis.ap.ngrok.io/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://www.liruilong.cn/projects/tava/"><img src="teasers/Li2022.jpg"/></a></td>
<td align="left" width=550><em>TAVA: Template-free Animatable Volumetric Actors</em><br>
<a href="https://www.liruilong.cn/"><font size=2.5>Ruilong Li</font></a>, 
<font size=2.5>Julian Tanke</font>, 
<a href="https://minhpvo.github.io/"><font size=2.5>Minh Vo</font></a>, 
<a href="https://zollhoefer.com/"><font size=2.5>Michael Zollhoefer</font></a>, 
<a href="https://pages.iai.uni-bonn.de/gall_juergen/"><font size=2.5>Jurgen Gall</font></a>, 
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://christophlassner.de/"><font size=2.5>Christoph Lassner</font></a><br>
<font size=2.5>In ECCV 2022 </font><br>
<a href="https://arxiv.org/pdf/2206.08929.pdf"><img src="data/paper.png"></a> 
<a href="https://www.liruilong.cn/projects/tava/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/EBGU/Selfee/"><img src="teasers/Jia2022.jpg"/></a></td>
<td align="left" width=550><em>Selfee, Self-supervised Features Extraction of animal behaviors</em><br>
<font size=2.5>Yinjun Jia</font>, 
<font size=2.5>Shuaishuai Li</font>, 
<font size=2.5>Xuan Guo</font>, 
<font size=2.5>Bo Lei</font>, 
<font size=2.5>Junqiang Hu</font>, 
<font size=2.5>Xiao-Hong Xu</font>, 
<font size=2.5>Wei Zhang</font><br>
<font size=2.5>In Elife 2022 </font><br>
<a href="https://elifesciences.org/articles/76218"><img src="data/paper.png"></a> 
<a href="https://github.com/EBGU/Selfee/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://nely-epfl.github.io/NeuroMechFly/"><img src="teasers/Rios2021.jpg"/></a></td>
<td align="left" width=550><em>NeuroMechFly, a neuromechanical model of adult Drosophila melanogaster</em><br>
<font size=2.5>Victor Lobato Rios</font>, 
<font size=2.5>Pembe Gizem Ozdil</font>, 
<font size=2.5>Shravan Tata Ramalingasetty</font>, 
<font size=2.5>Jonathan Arreguit</font>, 
<a href="https://www.epfl.ch/labs/biorob/people/ijspeert/"><font size=2.5>Auke Jan Ijspeert</font></a>, 
<a href="https://people.epfl.ch/pavan.ramdya"><font size=2.5>Pavan Ramdya</font></a><br>
<font size=2.5>In Nature Methods 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2021.04.17.440214v2"><img src="data/paper.png"></a> 
<a href="https://nely-epfl.github.io/NeuroMechFly/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Blau2022.jpg"/></a></td>
<td align="left" width=550><em>SemiMultiPose: A Semi-supervised Multi-animal Pose Estimation Framework</em><br>
<font size=2.5>Ari Blau</font>, 
<font size=2.5>Christoph Gebhardt</font>, 
<font size=2.5>Andres Bendesky</font>, 
<font size=2.5>Liam Paninski</font>, 
<a href="https://sites.google.com/site/anqiwuresearch/publica"><font size=2.5>Anqi Wu</font></a><br>
<font size=2.5>In arxiv 2022 </font><br>
<a href="https://arxiv-export1.library.cornell.edu/pdf/2204.07072"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://sutdcv.github.io/Animal-Kingdom/"><img src="teasers/Ng2022.jpg"/></a></td>
<td align="left" width=550><em>Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding</em><br>
<font size=2.5>Xun Long Ng</font>, 
<font size=2.5>Kian Eng Ong</font>, 
<font size=2.5>Qichen Zheng</font>, 
<font size=2.5>Yun Ni</font>, 
<font size=2.5>Si Yong Yeo</font>, 
<a href="https://istd.sutd.edu.sg/people/faculty/liu-jun"><font size=2.5>Jun Liu</font></a><br>
<font size=2.5>In CVPR 2022 </font>(<b><font size=2.5>oral</font></b>)<br>
<a href="https://www.researchgate.net/publication/359816954_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_Understanding"><img src="data/paper.png"></a> 
<a href="https://sutdcv.github.io/Animal-Kingdom/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://deeplabcut.github.io/DeepLabCut/docs/intro.html?msclkid=94c16acabc5a11ec80efaafc9aaeb28c"><img src="teasers/Lauer2022.jpg"/></a></td>
<td align="left" width=550><em>Multi-animal pose estimation, identification and tracking with DeepLabCut</em><br>
<font size=2.5>Jessy Lauer</font>, 
<font size=2.5>Mu Zhou</font>, 
<font size=2.5>Shaokai Ye</font>, 
<font size=2.5>William Menegas</font>, 
<font size=2.5>Steffen Schneider</font>, 
<font size=2.5>Tanmay Nath</font>, 
<font size=2.5>Mohammed Mostafizur Rahman</font>, 
<font size=2.5>Valentina Di Santo</font>, 
<font size=2.5>Daniel Soberanes</font>, 
<font size=2.5>Guoping Feng</font>, 
<font size=2.5>Venkatesh N. Murthy</font>, 
<font size=2.5>George Lauder</font>, 
<font size=2.5>Catherine Dulac</font>, 
<font size=2.5>Mackenzie Weygandt Mathis</font>, 
<a href="https://www.mathislab.org/people"><font size=2.5>Alexander Mathis</font></a><br>
<font size=2.5>In Nature Methods 2022 </font><br>
<a href="https://www.nature.com/articles/s41592-022-01443-0"><img src="data/paper.png"></a> 
<a href="https://deeplabcut.github.io/DeepLabCut/docs/intro.html?msclkid=94c16acabc5a11ec80efaafc9aaeb28c"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://sleap.ai/"><img src="teasers/Pereira2022.jpg"/></a></td>
<td align="left" width=550><em>SLEAP: A deep learning system for multi-animal pose tracking</em><br>
<a href="https://biophysics.princeton.edu/people/talmo-pereira"><font size=2.5>Talmo D. Pereira</font></a>, 
<font size=2.5>Nathaniel Tabris</font>, 
<font size=2.5>Arie Matsliah</font>, 
<font size=2.5>David M. Turner</font>, 
<font size=2.5>Junyu Li</font>, 
<font size=2.5>Shruthi Ravindranath</font>, 
<font size=2.5>Eleni S. Papadoyannis</font>, 
<font size=2.5>Edna Normand</font>, 
<font size=2.5>David S. Deutsch</font>, 
<font size=2.5>Z. Yan Wang</font>, 
<font size=2.5>Grace McKenzie-Smith</font>, 
<font size=2.5>Catalin C. Mitelut</font>, 
<font size=2.5>Marielisa Diez Castro</font>, 
<font size=2.5>John D'Uva</font>, 
<font size=2.5>Mikhail Kislin</font>, 
<a href="https://www.saneslab.com/"><font size=2.5>Dan H. Sanes</font></a>, 
<a href="https://lsi.princeton.edu/sarah-d-kocher"><font size=2.5>Sarah D. Kocher</font></a>, 
<a href="https://scholar.princeton.edu/wanglab/about"><font size=2.5>Samuel S.-H. Wang</font></a>, 
<a href="https://www.falknerlab.com/"><font size=2.5>Annegret L. Falkner</font></a>, 
<a href="https://molbio.princeton.edu/people/joshua-w-shaevitz"><font size=2.5>Joshua W. Shaevitz</font></a>, 
<a href="https://murthylab.princeton.edu/mala-murthy"><font size=2.5>Mala Murthy</font></a><br>
<font size=2.5>In Nature Methods 2022 </font><br>
<a href="https://www.nature.com/articles/s41592-022-01426-1"><img src="data/paper.png"></a> 
<a href="https://sleap.ai/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://barc.is.tue.mpg.de/"><img src="teasers/Ruegg2022.jpg"/></a></td>
<td align="left" width=550><em>BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information</em><br>
<a href="https://ps.is.mpg.de/person/nrueegg"><font size=2.5>Nadine Ruegg</font></a>, 
<a href="https://ps.is.mpg.de/person/szuffi"><font size=2.5>Silvia Zuffi</font></a>, 
<font size=2.5>Konrad Schindler</font>, 
<a href="https://ps.is.mpg.de/person/black"><font size=2.5>Michael J. Black</font></a><br>
<font size=2.5>In CVPR 2022 </font><br>
<a href="https://arxiv.org/pdf/2203.15536.pdf"><img src="data/paper.png"></a> 
<a href="https://barc.is.tue.mpg.de/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Luo2022.jpg"/></a></td>
<td align="left" width=550><em>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</em><br>
<font size=2.5>Haimin Luo</font>, 
<font size=2.5>Teng Xu</font>, 
<font size=2.5>Yuheng Jiang</font>, 
<font size=2.5>Chenglin Zhou</font>, 
<font size=2.5>Qiwei Qiu</font>, 
<font size=2.5>Yingliang Zhang</font>, 
<font size=2.5>Wei Yang</font>, 
<a href="https://www.xu-lan.com/"><font size=2.5>Lan Xu</font></a>, 
<a href="https://vic.shanghaitech.edu.cn/vrvc/en/people/jingyi-yu/"><font size=2.5>Jingyi Yu</font></a><br>
<font size=2.5>In TOG (Siggraph) 2022 </font><br>
<a href="https://arxiv.org/abs/2202.05628"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/chrelli/3DDD_social_mouse_tracker/"><img src="teasers/Ebbesen2020.jpg"/></a></td>
<td align="left" width=550><em>Automatic mapping of multiplexed social receptive fields by deep learning and GPU-accelerated 3D videography</em><br>
<font size=2.5>Christian L. Ebbesen</font>, 
<a href="http://froemkelab.med.nyu.edu/people"><font size=2.5>Robert C. Froemke</font></a><br>
<font size=2.5>In Nature Communications 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2020.05.21.109629v2"><img src="data/paper.png"></a> 
<a href="https://github.com/chrelli/3DDD_social_mouse_tracker/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://banmo-www.github.io/"><img src="teasers/Yang2022.jpg"/></a></td>
<td align="left" width=550><em>BANMo: Building Animatable 3D Neural Models from Many Casual Videos</em><br>
<a href="https://gengshan-y.github.io/"><font size=2.5>Gengshan Yang</font></a>, 
<a href="https://minhpvo.github.io/"><font size=2.5>Minh Vo</font></a>, 
<a href="https://nneverova.github.io/"><font size=2.5>Natalia Neverova</font></a>, 
<a href="https://www.cs.cmu.edu/~deva/"><font size=2.5>Deva Ramanan</font></a>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a>, 
<a href="https://jhugestar.github.io/"><font size=2.5>Hanbyul Joo</font></a><br>
<font size=2.5>In CVPR 2022 </font><br>
<a href="https://arxiv.org/abs/2112.12761"><img src="data/paper.png"></a> 
<a href="https://banmo-www.github.io/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Gunel2021.jpg"/></a></td>
<td align="left" width=550><em>Overcoming the Domain Gap in Neural Action Representations</em><br>
<font size=2.5>Semih Gunel</font>, 
<font size=2.5>Florian Aymanns</font>, 
<font size=2.5>Sina Honari</font>, 
<font size=2.5>Pavan Ramdya</font>, 
<font size=2.5>Pascal Fua</font><br>
<font size=2.5>In arxiv 2021 </font><br>
<a href="https://arxiv.org/pdf/2112.01176.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Nayak2022.jpg"/></a></td>
<td align="left" width=550><em>Incremental Learning for Animal Pose Estimation using RBF k-DPP</em><br>
<font size=2.5>Gaurav Kumar Nayak</font>, 
<a href="https://het-shah.github.io/"><font size=2.5>Het Shah</font></a>, 
<a href="http://visual-computing.in/wp-content/uploads/2017/08/anirban-chakraborty.html"><font size=2.5>Anirban Chakraborty</font></a><br>
<font size=2.5>In BMVC 2021 </font><br>
<a href="https://arxiv.org/pdf/2110.13598.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Jiang2020.jpg"/></a></td>
<td align="left" width=550><em>Muti-view Mouse Social Behaviour Recognition with Deep Graphic Model</em><br>
<font size=2.5>Zheheng Jiang</font>, 
<font size=2.5>Feixiang Zhou</font>, 
<font size=2.5>Aite Zhao</font>, 
<font size=2.5>Xin Li</font>, 
<font size=2.5>Ling Li</font>, 
<a href="https://dl.acm.org/profile/81100159571"><font size=2.5>Dacheng Tao</font></a>, 
<a href="http://www.dcs.bbk.ac.uk/~xuelong/"><font size=2.5>Xuelong Li</font></a>, 
<a href="https://www2.le.ac.uk/departments/informatics/people/huiyu-zhou"><font size=2.5>Huiyu Zhou</font></a><br>
<font size=2.5>In TIP 2021 </font><br>
<a href="https://arxiv.org/pdf/2011.02451.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Han2021.jpg"/></a></td>
<td align="left" width=550><em>MouseVenue3D: A Markerless Three-Dimension Behavioral Tracking System for Matching Two-Photon Brain Imaging in Free-Moving Mice</em><br>
<font size=2.5>Yaning Han</font>, 
<font size=2.5>Kang Huang</font>, 
<font size=2.5>Ke Chen</font>, 
<font size=2.5>Hongli Pan</font>, 
<font size=2.5>Furong Ju</font>, 
<font size=2.5>Yueyue Long</font>, 
<font size=2.5>Gao Gao</font>, 
<font size=2.5>Runlong Wu</font>, 
<font size=2.5>Aimin Wang</font>, 
<font size=2.5>Liping Wang</font>, 
<font size=2.5>Pengfei Wei</font><br>
<font size=2.5>In Neuroscience Bulletin 2021 </font><br>
<a href="https://link.springer.com/article/10.1007%2Fs12264-021-00778-6"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-brain-orange" align="bottom"><img src="https://img.shields.io/badge/topic-brain-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Straw2010.jpg"/></a></td>
<td align="left" width=550><em>Multi-camera real-time threedimensional tracking of multiple flying animals</em><br>
<a href="https://strawlab.org/"><font size=2.5>Andrew D. Straw</font></a>, 
<font size=2.5>Kristin Branson</font>, 
<font size=2.5>Titus R. Neumann</font>, 
<a href="https://www.bbe.caltech.edu/people/michael-h-dickinson"><font size=2.5>Michael H. Dickinson</font></a><br>
<font size=2.5>In J. R. Soc. Interface 2011 </font><br>
<a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2010.0230"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://neuroethology.github.io/MARS/"><img src="teasers/Segalin2021.jpg"/></a></td>
<td align="left" width=550><em>The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice</em><br>
<font size=2.5>Cristina Segalin</font>, 
<font size=2.5>Jalani Williams</font>, 
<font size=2.5>Tomomi Karigo</font>, 
<font size=2.5>May Hui</font>, 
<font size=2.5>Moriel Zelikowsky</font>, 
<font size=2.5>Jennifer J Sun</font>, 
<font size=2.5>Pietro Perona</font>, 
<font size=2.5>David J Anderson</font>, 
<font size=2.5>Ann Kennedy</font><br>
<font size=2.5>In eLife 2021 </font><br>
<a href="https://elifesciences.org/articles/63720.pdf"><img src="data/paper.png"></a> 
<a href="https://neuroethology.github.io/MARS/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Sun2021-2.jpg"/></a></td>
<td align="left" width=550><em>Self-Supervised Keypoint Discovery in Behavioral Videos</em><br>
<font size=2.5>Jennifer J. Sun</font>, 
<font size=2.5>Serim Ryou</font>, 
<font size=2.5>Roni Goldshmid</font>, 
<font size=2.5>Brandon Weissbourd</font>, 
<font size=2.5>John Dabiri</font>, 
<font size=2.5>David J. Anderson</font>, 
<font size=2.5>Ann Kennedy</font>, 
<font size=2.5>Yisong Yue</font>, 
<font size=2.5>Pietro Perona</font><br>
<font size=2.5>In arxiv 2021 </font><br>
<a href="https://arxiv.org/pdf/2112.05121.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications"><img src="teasers/Sun2021.jpg"/></a></td>
<td align="left" width=550><em>The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions</em><br>
<font size=2.5>Jennifer J. Sun</font>, 
<font size=2.5>Tomomi Karigo</font>, 
<font size=2.5>Dipam Chakraborty</font>, 
<font size=2.5>Sharada P. Mohanty</font>, 
<font size=2.5>David J. Anderson</font>, 
<font size=2.5>Pietro Perona</font>, 
<font size=2.5>Yisong Yue</font>, 
<font size=2.5>Ann Kennedy</font><br>
<font size=2.5>In NeurIPS (Dataset & Benchmarks) 2021 </font><br>
<a href="https://deepai.org/publication/the-multi-agent-behavior-dataset-mouse-dyadic-social-interactions"><img src="data/paper.png"></a> 
<a href="https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://sites.google.com/view/task-programming"><img src="teasers/Sun2021-1.jpg"/></a></td>
<td align="left" width=550><em>Task Programming: Learning Data Efficient Behavior Representations</em><br>
<font size=2.5>Jennifer J. Sun</font>, 
<font size=2.5>Ann Kennedy</font>, 
<font size=2.5>Eric Zhan</font>, 
<font size=2.5>David J. Anderson</font>, 
<font size=2.5>Yisong Yue</font>, 
<font size=2.5>Pietro Perona</font><br>
<font size=2.5>In CVPR 2021 </font>(<b><font size=2.5>Best Student Paper Award</font></b>)<br>
<a href="http://export.arxiv.org/pdf/2011.13917"><img src="data/paper.png"></a> 
<a href="http://sites.google.com/view/task-programming"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Li2021-3.jpg"/></a></td>
<td align="left" width=550><em>Coarse-to-fine Animal Pose and Shape Estimation</em><br>
<a href="https://chaneyddtt.github.io/"><font size=2.5>Chen Li</font></a>, 
<a href="https://www.comp.nus.edu.sg/~leegh/"><font size=2.5>Gim Hee Lee</font></a><br>
<font size=2.5>In NeurIPS 2021 </font><br>
<a href="https://arxiv.org/pdf/2111.08176.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kim2021.jpg"/></a></td>
<td align="left" width=550><em>Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise</em><br>
<font size=2.5>Kim Youwang</font>, 
<font size=2.5>Kim Ji-Yeon</font>, 
<font size=2.5>Kyungdon Joo</font>, 
<font size=2.5>Tae-Hyun Oh</font><br>
<font size=2.5>In BMVC 2021 </font><br>
<a href="https://arxiv.org/pdf/2111.02450.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://anipose.readthedocs.io/en/latest/index.html"><img src="teasers/Karashchuk2021.jpg"/></a></td>
<td align="left" width=550><em>Anipose: a toolkit for robust markerless 3D pose estimation</em><br>
<a href="https://github.com/lambdaloop"><font size=2.5>Pierre Karashchuk</font></a>, 
<font size=2.5>Katie L. Rupp</font>, 
<font size=2.5>Evyn S. Dickinson</font>, 
<font size=2.5>SarahWalling-Bell</font>, 
<font size=2.5>Elischa Sanders</font>, 
<a href="https://www.salk.edu/scientist/eiman-azim/"><font size=2.5>Eiman Azim</font></a>, 
<a href="https://www.bingbrunton.com/"><font size=2.5>Bingni W. Brunton</font></a>, 
<a href="http://faculty.washington.edu/tuthill/"><font size=2.5>John C. Tuthill</font></a><br>
<font size=2.5>In Cell Reports (Resource) 2021 </font><br>
<a href="https://www.sciencedirect.com/science/article/pii/S2211124721011797?via%3Dihub"><img src="data/paper.png"></a> 
<a href="https://anipose.readthedocs.io/en/latest/index.html"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Le2021.jpg"/></a></td>
<td align="left" width=550><em>Multimodal-based Scene-Aware Framework for Aquatic Animal Segmentation</em><br>
<font size=2.5>Minh-Quan Le</font>, 
<font size=2.5>Trung-Nghia Le</font>, 
<font size=2.5>Tam V. Nguyen</font>, 
<font size=2.5>Isao Echizen</font>, 
<font size=2.5>Minh-Triet Tran</font><br>
<font size=2.5>In arxiv 2021 </font><br>
<a href="https://arxiv.org/pdf/2112.06193.pdf"><img src="data/paper.png"></a> 
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Fujii2021.jpg"/></a></td>
<td align="left" width=550><em>Learning interaction rules from multi-animal trajectories via augmented behavioral models</em><br>
<font size=2.5>Keisuke Fujii</font>, 
<font size=2.5>Naoya Takeishi</font>, 
<font size=2.5>Kazushi Tsutsui</font>, 
<font size=2.5>Emyo Fujioka</font>, 
<font size=2.5>Nozomi Nishiumi</font>, 
<font size=2.5>Ryoya Tanaka</font>, 
<font size=2.5>Mika Fukushiro</font>, 
<font size=2.5>Kaoru Ide</font>, 
<font size=2.5>Hiroyoshi Kohno</font>, 
<font size=2.5>Ken Yoda</font>, 
<font size=2.5>Susumu Takahashi</font>, 
<font size=2.5>Shizuko Hiryu</font>, 
<a href="http://en.kawahara-lab.org/~kawahara/"><font size=2.5>Yoshinobu Kawahara</font></a><br>
<font size=2.5>In NeurIPS 2021 </font><br>
<a href="https://arxiv.org/pdf/2107.05326.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://figshare.com/projects/The_PAIR-R24M_Dataset_for_Multi-animal_3D_Pose_Estimation/115587"><img src="teasers/Marshall2021-2.jpg"/></a></td>
<td align="left" width=550><em>The PAIR-R24M Dataset for Multi-animal 3D Pose Estimation</em><br>
<font size=2.5>Jesse Marshall</font>, 
<font size=2.5>Ugne Klibaite</font>, 
<font size=2.5>Amanda Gellis</font>, 
<font size=2.5>Diego Aldarondo</font>, 
<a href="https://oeb.harvard.edu/people/bence-p-olveczky"><font size=2.5>Bence O¨lveczky</font></a>, 
<font size=2.5>Tim Dunn</font><br>
<font size=2.5>In NeurIPS 2021 </font><br>
<a href="https://openreview.net/pdf?id=-wVVl_UPr8"><img src="data/paper.png"></a> 
<a href="https://figshare.com/projects/The_PAIR-R24M_Dataset_for_Multi-animal_3D_Pose_Estimation/115587"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Vidal2021.jpg"/></a></td>
<td align="left" width=550><em>Across-animal odor decoding by probabilistic manifold alignment</em><br>
<font size=2.5>Pedro Herrero-Vidal</font>, 
<font size=2.5>Dmitry Rinberg</font>, 
<a href="https://as.nyu.edu/content/nyu-as/as/faculty/cristina-savin.html"><font size=2.5>Cristina Savin</font></a><br>
<font size=2.5>In NeurIPS 2021 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2021.06.06.447279v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/rabbityl/DeformingThings4D"><img src="teasers/Li2021-2.jpg"/></a></td>
<td align="left" width=550><em>4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface</em><br>
<font size=2.5>Yang Li</font>, 
<font size=2.5>Hikari Takehara</font>, 
<font size=2.5>Takafumi Taketomi</font>, 
<font size=2.5>Bo Zheng</font>, 
<a href="https://niessnerlab.org/"><font size=2.5>Matthias Nießner</font></a><br>
<font size=2.5>In ICCV 2021 </font><br>
<a href="https://arxiv.org/pdf/2105.01905.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/rabbityl/DeformingThings4D"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Shapovalov2021.jpg"/></a></td>
<td align="left" width=550><em>DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension</em><br>
<font size=2.5>Roman Shapovalov</font>, 
<font size=2.5>David Novotny</font>, 
<font size=2.5>Benjamin Graham</font>, 
<font size=2.5>Patrick Labatut</font>, 
<font size=2.5>Andrea Vedaldi</font><br>
<font size=2.5>In ICCV 2021 </font><br>
<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Shapovalov_DensePose_3D_Lifting_Canonical_Surface_Maps_of_Articulated_Objects_to_ICCV_2021_paper.html"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ebbesen2021.jpg"/></a></td>
<td align="left" width=550><em>Body language signals for rodent social communication</em><br>
<font size=2.5>Christian L. Ebbesen</font>, 
<a href="https://med.nyu.edu/faculty/robert-c-froemke"><font size=2.5>Robert C. Froemke</font></a><br>
<font size=2.5>In Current Opinion in Neurobiology 2021 </font><br>
<a href="https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC8243782&blobtype=pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Langford2010.jpg"/></a></td>
<td align="left" width=550><em>coding of facial expressions of pain in the laboratory mouse</em><br>
<font size=2.5>Dale J Langford</font>, 
<font size=2.5>Andrea L Bailey</font>, 
<font size=2.5>Mona Lisa Chanda</font>, 
<font size=2.5>Sarah E Clarke</font>, 
<font size=2.5>Tanya E Drummond</font>, 
<font size=2.5>Stephanie Echols</font>, 
<font size=2.5>Sarah Glick</font>, 
<font size=2.5>Joelle Ingrao</font>, 
<font size=2.5>Tammy Klassen-Ross</font>, 
<font size=2.5>Michael L LaCroix-Fralish</font>, 
<font size=2.5>Lynn Matsumiya</font>, 
<font size=2.5>Robert E Sorge</font>, 
<font size=2.5>Susana G Sotocinal</font>, 
<font size=2.5>John M Tabaka</font>, 
<font size=2.5>David Wong</font>, 
<font size=2.5>Arn M J M van den Maagdenberg</font>, 
<font size=2.5>Michel D Ferrari</font>, 
<font size=2.5>Kenneth D Craig</font>, 
<font size=2.5>Jeffrey S Mogil</font><br>
<font size=2.5>In Nature Methods (brief communications) 2010 </font><br>
<a href="https://www.nature.com/articles/nmeth.1455"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/AlexTheBad/AP-10K"><img src="teasers/Yu2021.jpg"/></a></td>
<td align="left" width=550><em>AP-10K: A Benchmark for Animal Pose Estimation in the Wild</em><br>
<font size=2.5>Hang Yu</font>, 
<font size=2.5>Yufei Xu</font>, 
<font size=2.5>Jing Zhang</font>, 
<font size=2.5>Wei Zhao</font>, 
<font size=2.5>Ziyu Guan</font>, 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html"><font size=2.5>Dacheng Tao</font></a><br>
<font size=2.5>In NeurIPS 2021 </font><br>
<a href="https://arxiv.org/pdf/2108.12617.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/AlexTheBad/AP-10K"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://deepimagej.github.io/deepimagej/"><img src="teasers/Mariscal2021.jpg"/></a></td>
<td align="left" width=550><em>DeepImageJ: A user-friendly environment to run deep learning models in ImageJ</em><br>
<font size=2.5>Estibaliz Gómez-de-Mariscal</font>, 
<font size=2.5>Carlos García-López-de-Haro</font>, 
<font size=2.5>Wei Ouyang</font>, 
<font size=2.5>Laurène Donati</font>, 
<font size=2.5>Emma Lundberg</font>, 
<font size=2.5>Michael Unser</font>, 
<font size=2.5>Arrate Muñoz-Barrutia</font>, 
<a href="https://people.epfl.ch/daniel.sage/?lang=en"><font size=2.5>Daniel Sage</font></a><br>
<font size=2.5>In Nature Methods (brief communication) 2021 </font><br>
<a href="https://www.nature.com/articles/s41592-021-01262-9.pdf"><img src="data/paper.png"></a> 
<a href="https://deepimagej.github.io/deepimagej/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://idtrackerai.readthedocs.io/en/latest/"><img src="teasers/Ferrero2019.jpg"/></a></td>
<td align="left" width=550><em>idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</em><br>
<a href="https://www.researchgate.net/profile/Francisco_Romero-Ferrero"><font size=2.5>Francisco Romero-Ferrero</font></a>, 
<font size=2.5>Mattia G. Bergomi</font>, 
<font size=2.5>Robert C. Hinz</font>, 
<font size=2.5>Francisco J. H. Heras</font>, 
<a href="http://www.neuro.fchampalimaud.org/en/person/276/"><font size=2.5>Gonzalo G. de Polavieja</font></a><br>
<font size=2.5>In Nature Methods (Brief Communication) 2019 </font><br>
<a href="https://arxiv.org/abs/1803.04351"><img src="data/paper.png"></a> 
<a href="https://idtrackerai.readthedocs.io/en/latest/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://openmonkeychallenge.com/"><img src="teasers/Yao2021.jpg"/></a></td>
<td align="left" width=550><em>OpenMonkeyChallenge: Dataset and Benchmark Challenges for Pose Tracking of Non-human Primates</em><br>
<font size=2.5>Yuan Yao</font>, 
<font size=2.5>Abhiraj Mohan</font>, 
<font size=2.5>Eliza Bliss-Moreau</font>, 
<font size=2.5>Kristine Coleman</font>, 
<font size=2.5>Sienna M. Freeman</font>, 
<font size=2.5>Christopher J. Machado</font>, 
<font size=2.5>Jessica Raper</font>, 
<font size=2.5>Jan Zimmermann</font>, 
<font size=2.5>Benjamin Y. Hayden</font>, 
<font size=2.5>Hyun Soo Park</font><br>
<font size=2.5>In IJCV 2022 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2021.09.08.459549v1.full.pdf"><img src="data/paper.png"></a> 
<a href="http://openmonkeychallenge.com/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Banik2021.jpg"/></a></td>
<td align="left" width=550><em>A Novel Dataset for Keypoint Detection of Quadruped Animals from Images</em><br>
<font size=2.5>Prianka Banik</font>, 
<font size=2.5>Lin Li</font>, 
<font size=2.5>Xishuang Dong</font><br>
<font size=2.5>In arxiv 2021 </font><br>
<a href="https://arxiv.org/pdf/2108.13958.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Neverova2021.jpg"/></a></td>
<td align="left" width=550><em>Discovering Relationships between Object Categories via Universal Canonical Maps</em><br>
<font size=2.5>Natalia Neverova</font>, 
<font size=2.5>Artsiom Sanakoyeu</font>, 
<font size=2.5>Patrick Labatut</font>, 
<font size=2.5>David Novotny</font>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a><br>
<font size=2.5>In CVPR 2021 </font><br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Neverova_Discovering_Relationships_Between_Object_Categories_via_Universal_Canonical_Maps_CVPR_2021_paper.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/NeLy-EPFL/LiftPose3D"><img src="teasers/Gosztolai2020.jpg"/></a></td>
<td align="left" width=550><em>LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals</em><br>
<font size=2.5>Adam Gosztolai</font>, 
<font size=2.5>Semih Gunel</font>, 
<font size=2.5>Marco Pietro Abrate</font>, 
<font size=2.5>Daniel Morales</font>, 
<font size=2.5>Victor Lobato Rios</font>, 
<a href="https://www.cs.ubc.ca/~rhodin/"><font size=2.5>Helge Rhodin</font></a>, 
<font size=2.5>Pascal Fua</font>, 
<a href="https://www.epfl.ch/labs/ramdya-lab/"><font size=2.5>Pavan Ramdya</font></a><br>
<font size=2.5>In Nature Methods 2021 </font><br>
<a href="https://www.nature.com/articles/s41592-021-01226-z.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/NeLy-EPFL/LiftPose3D"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/spoonsso/DANNCE"><img src="teasers/Dunn2021.jpg"/></a></td>
<td align="left" width=550><em>Geometric deep learning enables 3D kinematic profiling across species and environments</em><br>
<font size=2.5>Timothy W. Dunn</font>, 
<font size=2.5>Jesse D. Marshall</font>, 
<font size=2.5>Kyle S. Severson</font>, 
<font size=2.5>Diego E. Aldarondo</font>, 
<font size=2.5>David G. C. Hildebrand</font>, 
<font size=2.5>Selmaan N. Chettih</font>, 
<font size=2.5>William L. Wang</font>, 
<font size=2.5>Amanda J. Gellis</font>, 
<font size=2.5>David E. Carlson</font>, 
<font size=2.5>Dmitriy Aronov</font>, 
<font size=2.5>Winrich A. Freiwald</font>, 
<font size=2.5>Fan Wang</font>, 
<font size=2.5>Bence P. Ölveczky</font><br>
<font size=2.5>In Nature Methods 2021 </font><br>
<a href="https://www.nature.com/articles/s41592-021-01106-6.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/spoonsso/DANNCE"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://yufu-wang.github.io/aves/"><img src="teasers/Wang2021.jpg"/></a></td>
<td align="left" width=550><em>Birds of a Feather: Capturing Avian Shape Models from Images</em><br>
<a href="https://yufu-wang.github.io/"><font size=2.5>Yufu Wang</font></a>, 
<a href="https://www.seas.upenn.edu/~nkolot/"><font size=2.5>Nikos Kolotouros</font></a>, 
<a href="https://www.cis.upenn.edu/~kostas/"><font size=2.5>Kostas Daniilidis</font></a>, 
<a href="https://www.ocf.berkeley.edu/~badger/"><font size=2.5>Marc Badger</font></a><br>
<font size=2.5>In CVPR 2021 </font><br>
<a href="https://arxiv.org/pdf/2105.09396.pdf"><img src="data/paper.png"></a> 
<a href="https://yufu-wang.github.io/aves/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Corcoran2021.jpg"/></a></td>
<td align="left" width=550><em>ThruTracker: Open-Source Software for 2-D and 3-D Animal Video Tracking</em><br>
<font size=2.5>Aaron J. Corcoran</font>, 
<font size=2.5>Michael R. Schirmacher</font>, 
<font size=2.5>Eric Black</font>, 
<a href="https://biomech.web.unc.edu/people/"><font size=2.5>Tyson L. Hedrick</font></a><br>
<font size=2.5>In biorxiv 2021 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2021.05.12.443854v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://behavioratlas.tech/"><img src="teasers/Huang2020.jpg"/></a></td>
<td align="left" width=550><em>A Hierarchical 3D-motion Learning Framework for Animal Spontaneous Behavior Mapping</em><br>
<font size=2.5>Kang Huang</font>, 
<font size=2.5>Yaning Han</font>, 
<font size=2.5>Ke Chen</font>, 
<font size=2.5>Hongli Pan</font>, 
<font size=2.5>Wenling Yi</font>, 
<font size=2.5>Xiaoxi Li</font>, 
<font size=2.5>Siyuan Liu</font>, 
<a href="http://wanglab.siat.ac.cn/wanglab_en/index.php?a=lab_members"><font size=2.5>Liping Wang</font></a>, 
<a href="https://www.researchgate.net/profile/Pengfei_Wei2"><font size=2.5>Pengfei Wei</font></a><br>
<font size=2.5>In Nature Communications 2021 </font><br>
<a href="https://doi.org/10.1101/2020.09.14.295808"><img src="data/paper.png"></a> 
<a href="https://behavioratlas.tech/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Ziegler2021.jpg"/></a></td>
<td align="left" width=550><em>Big behavior: challenges and opportunities in a new era of deep behavior profiling</em><br>
<font size=2.5>Lukas von Ziegler</font>, 
<font size=2.5>Oliver Sturman</font>, 
<font size=2.5>Johannes Bohacek</font><br>
<font size=2.5>In Neuropsychopharmacology 2021 </font><br>
<a href="https://www.nature.com/articles/s41386-020-0751-7"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/ubcbraincircuits/mCBF"><img src="teasers/Bolanos2021.jpg"/></a></td>
<td align="left" width=550><em>A three-dimensional virtual mouse generates synthetic training data for behavioral analysis</em><br>
<font size=2.5>Luis A. Bolaños</font>, 
<font size=2.5>Dongsheng Xiao</font>, 
<font size=2.5>Nancy L. Ford</font>, 
<font size=2.5>Jeff M. LeDue</font>, 
<font size=2.5>Pankaj K. Gupta</font>, 
<font size=2.5>Carlos Doebeli</font>, 
<font size=2.5>Hao Hu</font>, 
<font size=2.5>Helge Rhodin</font>, 
<font size=2.5>Timothy H. Murphy</font><br>
<font size=2.5>In Nature Methods (Brief Communication) 2021 </font>(<b><font size=2.5>cover</font></b>)<br>
<a href="https://www.nature.com/articles/s41592-021-01103-9"><img src="data/paper.png"></a> 
<a href="https://github.com/ubcbraincircuits/mCBF"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/SchwarzNeuroconLab/DeepLabStream"><img src="teasers/Schweihoff2021.jpg"/></a></td>
<td align="left" width=550><em>DeepLabStream enables closed-loop behavioral experiments using deep learning-based markerless, real-time posture detection</em><br>
<font size=2.5>Jens F. Schweihoff</font>, 
<font size=2.5>Matvey Loshakov</font>, 
<font size=2.5>Irina Pavlova</font>, 
<font size=2.5>Laura Kück</font>, 
<font size=2.5>Laura A. Ewell</font>, 
<font size=2.5>Martin K. Schwarz</font><br>
<font size=2.5>In Communications Biology 2021 </font><br>
<a href="https://www.nature.com/articles/s42003-021-01654-9"><img src="data/paper.png"></a> 
<a href="https://github.com/SchwarzNeuroconLab/DeepLabStream"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/chaneyddtt/UDA-Animal-Pose"><img src="teasers/Li2021.jpg"/></a></td>
<td align="left" width=550><em>From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation</em><br>
<a href="https://chaneyddtt.github.io/"><font size=2.5>Chen Li</font></a>, 
<a href="https://www.comp.nus.edu.sg/~leegh/"><font size=2.5>Gim Hee Lee</font></a><br>
<font size=2.5>In CVPR 2021 </font>(<b><font size=2.5>oral</font></b>)<br>
<a href="https://arxiv.org/pdf/2103.14843.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/chaneyddtt/UDA-Animal-Pose"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://lasr-google.github.io/"><img src="teasers/Yang2021.jpg"/></a></td>
<td align="left" width=550><em>LASR: Learning Articulated Shape Reconstruction from a Monocular Video</em><br>
<a href="https://gengshan-y.github.io/"><font size=2.5>Gengshan Yang</font></a>, 
<a href="https://deqings.github.io/"><font size=2.5>Deqing Sun</font></a>, 
<a href="https://varunjampani.github.io/"><font size=2.5>Varun Jampani</font></a>, 
<a href="https://people.csail.mit.edu/drdaniel/"><font size=2.5>Daniel Vlasic</font></a>, 
<a href="https://people.csail.mit.edu/fcole/"><font size=2.5>Forrester Cole</font></a>, 
<font size=2.5>Huiwen Chang</font>, 
<a href="http://www.cs.cmu.edu/~deva/"><font size=2.5>Deva Ramanan</font></a>, 
<a href="https://billf.mit.edu/"><font size=2.5>William T. Freeman</font></a>, 
<a href="https://people.csail.mit.edu/celiu/"><font size=2.5>Ce Liu</font></a><br>
<font size=2.5>In CVPR 2021 </font><br>
<a href="https://arxiv.org/pdf/2105.02976.pdf"><img src="data/paper.png"></a> 
<a href="https://lasr-google.github.io/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/DeepLabCut/DeepLabCut-live"><img src="teasers/Kane2020.jpg"/></a></td>
<td align="left" width=550><em>Real-time, low-latency closed-loop feedback using markerless posture tracking</em><br>
<font size=2.5>Gary Kane</font>, 
<font size=2.5>Gonçalo Lopes</font>, 
<font size=2.5>Jonny L. Saunders</font>, 
<a href="https://www.mathislab.org/people"><font size=2.5>Alexander Mathis</font></a>, 
<font size=2.5>Mackenzie W. Mathis</font><br>
<font size=2.5>In eLife 2020 </font>(<b><font size=2.5>Featured by Nature Methods</font></b>)<br>
<a href="https://elifesciences.org/articles/61909"><img src="data/paper.png"></a> 
<a href="https://github.com/DeepLabCut/DeepLabCut-live"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Michaiel2020.jpg"/></a></td>
<td align="left" width=550><em>Dynamics of gaze control during prey capture in freely moving mice</em><br>
<font size=2.5>Angie M Michaiel</font>, 
<font size=2.5>Elliott TT Abe</font>, 
<font size=2.5>Cristopher M Niell</font><br>
<font size=2.5>In eLife 2020 </font><br>
<a href="https://elifesciences.org/articles/57458"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Marshall2020.jpg"/></a></td>
<td align="left" width=550><em>Continuous Whole-Body 3D Kinematic Recordings across the Rodent Behavioral Repertoire</em><br>
<font size=2.5>Jesse D. Marshall</font>, 
<font size=2.5>Diego E. Aldarondo</font>, 
<font size=2.5>Timothy W. Dunn</font>, 
<font size=2.5>William L. Wang</font>, 
<font size=2.5>Gordon J. Berman</font>, 
<a href="https://oeb.harvard.edu/people/bence-p-olveczky"><font size=2.5>Bence P. O¨lveczky</font></a><br>
<font size=2.5>In Neuron 2020 </font><br>
<a href="https://www.cell.com/neuron/fulltext/S0896-6273(20)30894-1?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627320308941%3Fshowall%3Dtrue"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Geuther2019.jpg"/></a></td>
<td align="left" width=550><em>Robust mouse tracking in complex environments using neural networks</em><br>
<font size=2.5>Brian Q. Geuther</font>, 
<font size=2.5>Sean P. Deats</font>, 
<font size=2.5>Kai J. Fox</font>, 
<font size=2.5>Steve A. Murray</font>, 
<font size=2.5>Robert E. Braun</font>, 
<font size=2.5>Jacqueline K. White</font>, 
<font size=2.5>Elissa J. Chesler</font>, 
<font size=2.5>Cathleen M. Lutz</font>, 
<font size=2.5>Vivek Kumar</font><br>
<font size=2.5>In Communications Biology 2019 </font><br>
<a href="https://www.nature.com/articles/s42003-019-0362-1"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/ZexinChen/AlphaTracker"><img src="teasers/Chen2020.jpg"/></a></td>
<td align="left" width=550><em>AlphaTracker: A Multi-Animal Tracking and Behavioral Analysis Tool</em><br>
<font size=2.5>Zexin Chen</font>, 
<font size=2.5>Ruihan Zhang</font>, 
<font size=2.5>Yu Eva Zhang</font>, 
<font size=2.5>Haowen Zhou</font>, 
<font size=2.5>Hao-Shu Fang</font>, 
<font size=2.5>Rachel R. Rock</font>, 
<font size=2.5>Aneesh Bal</font>, 
<font size=2.5>Nancy Padilla-Coreano</font>, 
<font size=2.5>Laurel Keyes</font>, 
<font size=2.5>Kay M. Tye</font>, 
<a href="https://www.mvig.org/"><font size=2.5>Cewu Lu</font></a><br>
<font size=2.5>In biorxiv 2020 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2020.12.04.405159v1.full.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/ZexinChen/AlphaTracker"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://drive.google.com/drive/folders/1W79W1m1JQNvL9bDoqBPhwOXqtyz0gc24?usp=sharing"><img src="teasers/Wu2020-1.jpg"/></a></td>
<td align="left" width=550><em>Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking</em><br>
<font size=2.5>Anqi Wu</font>, 
<font size=2.5>E. Kelly Buchanan</font>, 
<font size=2.5>Matthew Whiteway</font>, 
<font size=2.5>Michael Schartner</font>, 
<font size=2.5>Guido Meijer</font>, 
<font size=2.5>Jean-Paul Noel</font>, 
<font size=2.5>Erica Rodriguez</font>, 
<font size=2.5>Claire Everett</font>, 
<font size=2.5>Amy Norovich</font>, 
<font size=2.5>Evan Schaffer</font>, 
<font size=2.5>Neeli Mishra</font>, 
<font size=2.5>C. Daniel Salzman</font>, 
<font size=2.5>Dora Angelaki</font>, 
<font size=2.5>Andrés Bendesky</font>, 
<font size=2.5>The International Brain Laboratory</font>, 
<font size=2.5>John Cunningham</font>, 
<a href="http://www.stat.columbia.edu/~liam/"><font size=2.5>Liam Paninski</font></a><br>
<font size=2.5>In NeurIPS 2020 </font><br>
<a href="https://www.biorxiv.org/content/biorxiv/early/2020/08/22/2020.08.20.259705.full.pdf"><img src="data/paper.png"></a> 
<a href="https://drive.google.com/drive/folders/1W79W1m1JQNvL9bDoqBPhwOXqtyz0gc24?usp=sharing"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Nilsson2020.jpg"/></a></td>
<td align="left" width=550><em>Simple Behavioral Analysis (SimBA)-an open source toolkit for computer classification of complex social behaviors in experimental animals</em><br>
<font size=2.5>Simon RO Nilsson</font>, 
<font size=2.5>Nastacia L. Goodwin</font>, 
<font size=2.5>Jia Jie Choong</font>, 
<font size=2.5>Sophia Hwang</font>, 
<font size=2.5>Hayden R Wright</font>, 
<font size=2.5>Zane C Norville</font>, 
<font size=2.5>Xiaoyu Tong</font>, 
<font size=2.5>Dayu Lin</font>, 
<font size=2.5>Brandon S. Bentzley</font>, 
<font size=2.5>Neir Eshel</font>, 
<font size=2.5>Ryan J McLaughlin</font>, 
<a href="https://goldenneurolab.com/people"><font size=2.5>Sam A. Golden</font></a><br>
<font size=2.5>In biorxiv 2020 </font><br>
<a href="https://doi.org/10.1101/2020.04.19.049452"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://sleap.ai/"><img src="teasers/Pereira2020.jpg"/></a></td>
<td align="left" width=550><em>SLEAP: Multi-animal pose tracking</em><br>
<a href="https://biophysics.princeton.edu/people/talmo-pereira"><font size=2.5>Talmo D. Pereira</font></a>, 
<font size=2.5>Nathaniel Tabris</font>, 
<font size=2.5>Junyu Li</font>, 
<font size=2.5>Shruthi Ravindranath</font>, 
<font size=2.5>Eleni S. Papadoyannis</font>, 
<font size=2.5>Z. Yan Wang</font>, 
<font size=2.5>David M. Turner</font>, 
<font size=2.5>Grace McKenzie-Smith</font>, 
<font size=2.5>Sarah D. Kocher</font>, 
<font size=2.5>Annegret L. Falkner</font>, 
<font size=2.5>Joshua W. Shaevitz</font>, 
<a href="https://murthylab.princeton.edu/mala-murthy"><font size=2.5>Mala Murthy</font></a><br>
<font size=2.5>In biorxiv 2020 </font><br>
<a href="https://doi.org/10.1101/2020.08.31.276246"><img src="data/paper.png"></a> 
<a href="https://sleap.ai/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://lmb.informatik.uni-freiburg.de/projects/freipose/"><img src="teasers/Zimmermann2020.jpg"/></a></td>
<td align="left" width=550><em>FreiPose: A Deep Learning Framework for Precise Animal Motion Capture in 3D Spaces</em><br>
<a href="https://lmb.informatik.uni-freiburg.de/people/zimmermc/"><font size=2.5>Christian Zimmermann</font></a>, 
<font size=2.5>Artur Schneider</font>, 
<font size=2.5>Mansour Alyahyay</font>, 
<a href="https://lmb.informatik.uni-freiburg.de/people/brox/"><font size=2.5>Thomas Brox</font></a>, 
<a href="https://www.optophysiology.uni-freiburg.de/labmembers/diester/"><font size=2.5>Ilka Diester</font></a><br>
<font size=2.5>In biorxiv 2020 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2020.02.27.967620v1"><img src="data/paper.png"></a> 
<a href="https://lmb.informatik.uni-freiburg.de/projects/freipose/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Zhang2020.jpg"/></a></td>
<td align="left" width=550><em>Multiview Supervision By Registration</em><br>
<font size=2.5>Yilun Zhang</font>, 
<a href="https://www-users.cs.umn.edu/~hspark/"><font size=2.5>Hyun Soo Park</font></a><br>
<font size=2.5>In WACV 2020 </font><br>
<a href="https://arxiv.org/abs/1811.11251"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Dolensek2020.jpg"/></a></td>
<td align="left" width=550><em>Facial expressions of emotion states and their neuronal correlates in mice</em><br>
<font size=2.5>Nejc Dolensek</font>, 
<font size=2.5>Daniel A. Gehrlach</font>, 
<a href="https://muckrack.com/alexandra-s-klein"><font size=2.5>Alexandra S. Klein</font></a>, 
<a href="https://www.neuro.mpg.de/gogolla"><font size=2.5>Nadine Gogolla</font></a><br>
<font size=2.5>In Science 2020 </font><br>
<a href="https://science.sciencemag.org/content/368/6486/89.full#:~:text=Facial%20expressions%20thus%20provide%20a%20means%20to%20infer,hormonal%2C%20and%20autonomic%20responses%20aimed%20at%20promoting%20survival."><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Fangbemi2020.jpg"/></a></td>
<td align="left" width=550><em>ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data</em><br>
<font size=2.5>Abassin Sourou Fangbemi</font>, 
<font size=2.5>Yi Fei Lu</font>, 
<font size=2.5>Mao Yuan Xu</font>, 
<font size=2.5>Xiao Wu Luo</font>, 
<font size=2.5>Alexis Rolland</font>, 
<font size=2.5>Chedy Raissi</font><br>
<font size=2.5>In SCA (ACM Siggraph/Eurographics Symposium on Computer Animation) 2020 </font><br>
<a href="https://export.arxiv.org/abs/2009.05389"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Mathis2020.jpg"/></a></td>
<td align="left" width=550><em>A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and Perspectives</em><br>
<a href="http://www.people.fas.harvard.edu/~amathis/"><font size=2.5>Alexander Mathis</font></a>, 
<font size=2.5>Steffen Schneider</font>, 
<font size=2.5>Jessy Lauer</font>, 
<a href="https://scholar.harvard.edu/mwamoroso/home"><font size=2.5>Mackenzie W. Mathis</font></a><br>
<font size=2.5>In Neuron 2020 </font><br>
<a href="https://arxiv.org/pdf/2009.00564.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Pereira2020-1.jpg"/></a></td>
<td align="left" width=550><em>Quantifying behavior to understand the brain</em><br>
<a href="https://biophysics.princeton.edu/people/talmo-pereira"><font size=2.5>Talmo D. Pereira</font></a>, 
<a href="https://molbiod.princeton.edu/people/joshua-w-shaevitz"><font size=2.5>Joshua W. Shaevitz</font></a>, 
<a href="htps://murthylab.princeton.edu/mala-murthy"><font size=2.5>Mala Murthy</font></a><br>
<font size=2.5>In Nature Neuroscience 2020 </font>(<b><font size=2.5>review article</font></b>)<br>
<a href="https://www.nature.com/articles/s41593-020-00734-z"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-review-orange" align="bottom"><img src="https://img.shields.io/badge/topic-review-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Nourizonoz2020.jpg"/></a></td>
<td align="left" width=550><em>EthoLoop: automated closed-loop neuroethology in naturalistic environments</em><br>
<font size=2.5>Ali Nourizonoz</font>, 
<font size=2.5>Robert Zimmermann</font>, 
<font size=2.5>Chun Lum Andy Ho</font>, 
<font size=2.5>Sebastien Pellat</font>, 
<font size=2.5>Yannick Ormen</font>, 
<font size=2.5>Clément Prévost-Solié</font>, 
<font size=2.5>Gilles Reymond</font>, 
<font size=2.5>Fabien Pifferi</font>, 
<font size=2.5>Fabienne Aujard</font>, 
<font size=2.5>Anthony Herrel</font>, 
<a href="https://www.unige.ch/medecine/neuf/en/"><font size=2.5>Daniel Huber</font></a><br>
<font size=2.5>In Nature Methods 2020 </font>(<b><font size=2.5>cover</font></b>)<br>
<a href="https://pubmed.ncbi.nlm.nih.gov/32994566/"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-mouse%20lemur-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse%20lemur-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a><a href="https://img.shields.io/badge/topic-brain-orange" align="bottom"><img src="https://img.shields.io/badge/topic-brain-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="openmonkeystudio.org"><img src="teasers/Bala2020.jpg"/></a></td>
<td align="left" width=550><em>Automated Markerless Pose Estimation in Freely Moving Macaques using OpenMonkeyStudio</em><br>
<font size=2.5>Praneet C. Bala</font>, 
<font size=2.5>Benjamin R. Eisenreich</font>, 
<font size=2.5>Seng Bum Michael Yoo</font>, 
<font size=2.5>Benjamin Y. Hayden</font>, 
<a href="https://www-users.cs.umn.edu/~hspark/"><font size=2.5>Hyun Soo Park</font></a>, 
<a href="https://med.umn.edu/bio/medical-discovery-teams/jan-zimmermann"><font size=2.5>Jan Zimmermann</font></a><br>
<font size=2.5>In Nature Communications 2020 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/2020.01.31.928861v1"><img src="data/paper.png"></a> 
<a href="openmonkeystudio.org"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://marcbadger.github.io/avian-mesh/"><img src="teasers/Badger2020.jpg"/></a></td>
<td align="left" width=550><em>3D Bird Reconstruction: a Dataset, Model, and Shape Recovery from a Single View</em><br>
<a href="https://www.ocf.berkeley.edu/~badger/"><font size=2.5>Marc Badger</font></a>, 
<a href="https://yufu-wang.github.io/"><font size=2.5>Yufu Wang</font></a>, 
<a href="https://www.seas.upenn.edu/~adarshm/"><font size=2.5>Adarsh Modh</font></a>, 
<a href="https://aperkes.github.io/"><font size=2.5>Ammon Perkes</font></a>, 
<a href="https://www.seas.upenn.edu/~nkolot/"><font size=2.5>Nikos Kolotouros</font></a>, 
<a href="http://pfrommer.us/"><font size=2.5>Bernd Pfrommer</font></a>, 
<a href="https://web.sas.upenn.edu/marcschmidtlab/pages/people/"><font size=2.5>Marc F. Schmidt</font></a>, 
<a href="https://www.cis.upenn.edu/~kostas/"><font size=2.5>Kostas Daniilidis</font></a><br>
<font size=2.5>In ECCV 2020 </font><br>
<a href="https://arxiv.org/abs/2008.06133"><img src="data/paper.png"></a> 
<a href="https://marcbadger.github.io/avian-mesh/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Biggs2020.jpg"/></a></td>
<td align="left" width=550><em>Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop</em><br>
<a href="http://mi.eng.cam.ac.uk/~bjb56/"><font size=2.5>Benjamin Biggs</font></a>, 
<a href="https://uk.linkedin.com/in/ollie-boyne"><font size=2.5>Oliver Boyne</font></a>, 
<a href="http://www.jjcvision.com/"><font size=2.5>James Charles</font></a>, 
<a href="https://www.microsoft.com/en-us/research/people/awf/"><font size=2.5>Andrew Fitzgibbon</font></a>, 
<a href="https://mi.eng.cam.ac.uk/~cipolla/"><font size=2.5>Roberto Cipolla</font></a><br>
<font size=2.5>In ECCV 2020 </font><br>
<a href="https://arxiv.org/abs/2007.11110"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://shubham-goel.github.io/ucmr/"><img src="teasers/Goel2020.jpg"/></a></td>
<td align="left" width=550><em>Shape and Viewpoint without Keypoints</em><br>
<a href="https://people.eecs.berkeley.edu/~shubham-goel/"><font size=2.5>Shubham Goel</font></a>, 
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://people.eecs.berkeley.edu/~malik/"><font size=2.5>Jitendra Malik</font></a><br>
<font size=2.5>In ECCV 2020 </font><br>
<a href="https://arxiv.org/pdf/2007.10982.pdf"><img src="data/paper.png"></a> 
<a href="https://shubham-goel.github.io/ucmr/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Shi2020.jpg"/></a></td>
<td align="left" width=550><em>Deep Cross-species Feature Learning for Animal Face Recognition via Residual Interspecies Equivariant Network</em><br>
<font size=2.5>Xiao Shi</font>, 
<font size=2.5>Chenxue Yang</font>, 
<font size=2.5>Xue Xia</font>, 
<a href="http://aii.caas.cn/bsgk/ywbm/nyxxjzsyb/xzbm3/203034.htm"><font size=2.5>Xiujuan Chai</font></a><br>
<font size=2.5>In ECCV 2020 </font><br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720664.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a><a href="https://img.shields.io/badge/animal-pig-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-pig-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/CAMERA-Bath/RGBD-Dog#:~:text=%20RGBD-Dog%3A%20Predicting%20Canine%20Pose%20from%20RGBD%20Sensors,5%20Citation.%20%206%20Contact.%20%20More%20"><img src="teasers/Kearney2020.jpg"/></a></td>
<td align="left" width=550><em>RGBD-Dog: Predicting Canine Pose from RGBD Sensors</em><br>
<a href="https://researchportal.bath.ac.uk/en/persons/sinead-kearney"><font size=2.5>Sinead Kearney</font></a>, 
<a href="https://wbli.me/"><font size=2.5>Wenbin Li</font></a>, 
<font size=2.5>Martin Parsons</font>, 
<a href="http://kimki.unist.ac.kr/"><font size=2.5>Kwang In Kim</font></a>, 
<a href="http://www.cs.bath.ac.uk/~dpc/"><font size=2.5>Darren Cosker</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kearney_RGBD-Dog_Predicting_Canine_Pose_from_RGBD_Sensors_CVPR_2020_paper.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/CAMERA-Bath/RGBD-Dog#:~:text=%20RGBD-Dog%3A%20Predicting%20Canine%20Pose%20from%20RGBD%20Sensors,5%20Citation.%20%206%20Contact.%20%20More%20"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-dog-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-dog-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://fdmaproject.wordpress.com/"><img src="teasers/Khan2020.jpg"/></a></td>
<td align="left" width=550><em>AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces</em><br>
<font size=2.5>Muhammad Haris Khan</font>, 
<font size=2.5>John McDonagh</font>, 
<font size=2.5>Salman Khan</font>, 
<font size=2.5>Muhammad Shahabuddin</font>, 
<font size=2.5>Aditya Arora</font>, 
<font size=2.5>Fahad Shahbaz Khan</font>, 
<font size=2.5>Ling Shao</font>, 
<a href="http://www.cs.nott.ac.uk/~pszyt/"><font size=2.5>Georgios Tzimiropoulos</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://arxiv.org/abs/1909.04951"><img src="data/paper.png"></a> 
<a href="https://fdmaproject.wordpress.com/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://nileshkulkarni.github.io/acsm/"><img src="teasers/Kulkarni2020.png"/></a></td>
<td align="left" width=550><em>Articulation Aware Canonical Surface Mapping</em><br>
<a href="https://nileshkulkarni.github.io/"><font size=2.5>Nilesh Kulkarni</font></a>, 
<a href="http://www.cs.cmu.edu/~abhinavg/"><font size=2.5>Abhinav Gupta</font></a>, 
<a href="http://web.eecs.umich.edu/~fouhey/"><font size=2.5>David F. Fouhey</font></a>, 
<a href="https://shubhtuls.github.io/"><font size=2.5>Shubham Tulsiani</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://arxiv.org/pdf/2004.00614.pdf"><img src="data/paper.png"></a> 
<a href="https://nileshkulkarni.github.io/acsm/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/siyliepfl/deformation-aware-unpaired-image-translation"><img src="teasers/Li2020.JPG"/></a></td>
<td align="left" width=550><em>Deformation-aware Unpaired Image Translation for Pose Estimation on Laboratory Animals</em><br>
<a href="https://siyliepfl.github.io/"><font size=2.5>Siyuan Li</font></a>, 
<a href="https://semihgunel.com/"><font size=2.5>Semih Gunel</font></a>, 
<font size=2.5>Mirela Ostrek</font>, 
<font size=2.5>Pavan Ramdya</font>, 
<font size=2.5>Pascal Fua</font>, 
<a href="https://www.cs.ubc.ca/~rhodin/"><font size=2.5>Helge Rhodin</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deformation-Aware_Unpaired_Image_Translation_for_Pose_Estimation_on_Laboratory_Animals_CVPR_2020_paper.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/siyliepfl/deformation-aware-unpaired-image-translation"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/JitengMu/Learning-from-Synthetic-Animals"><img src="teasers/Mu2020.JPG"/></a></td>
<td align="left" width=550><em>Learning from Synthetic Animals</em><br>
<font size=2.5>Jiteng Mu</font>, 
<a href="https://weichaoqiu.com/"><font size=2.5>Weichao Qiu</font></a>, 
<a href="https://www.cs.jhu.edu/hager/"><font size=2.5>Gregory Hager</font></a>, 
<a href="http://www.cs.jhu.edu/~ayuille/"><font size=2.5>Alan Yuille</font></a><br>
<font size=2.5>In CVPR 2020 </font>(<b><font size=2.5>oral</font></b>)<br>
<a href="https://arxiv.org/abs/1912.08265"><img src="data/paper.png"></a> 
<a href="https://github.com/JitengMu/Learning-from-Synthetic-Animals"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://vap.aau.dk/3d-zef/"><img src="teasers/Pedersen2020.jpg"/></a></td>
<td align="left" width=550><em>3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset</em><br>
<a href="https://vbn.aau.dk/en/persons/141158"><font size=2.5>Malte Pedersen</font></a>, 
<font size=2.5>Joakim Bruslund Haurum</font>, 
<a href="https://vbn.aau.dk/en/persons/138111"><font size=2.5>Stefan Hein Bengtson</font></a>, 
<a href="https://vbn.aau.dk/en/persons/103282"><font size=2.5>Thomas B. Moeslund</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pedersen_3D-ZeF_A_3D_Zebrafish_Tracking_Benchmark_Dataset_CVPR_2020_paper.pdf"><img src="data/paper.png"></a> 
<a href="https://vap.aau.dk/3d-zef/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://gdude.de/densepose-evolution/"><img src="teasers/Sanakoyeu2020.jpg"/></a></td>
<td align="left" width=550><em>Transferring Dense Pose to Proximal Animal Classes</em><br>
<a href="https://gdude.de/"><font size=2.5>Artsiom Sanakoyeu</font></a>, 
<a href="https://research.fb.com/people/khalidov-vasil/"><font size=2.5>Vasil Khalidov</font></a>, 
<a href="https://www.maureenmccarthyphd.com/"><font size=2.5>Maureen S. McCarthy</font></a>, 
<a href="https://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a>, 
<a href="https://nneverova.github.io/"><font size=2.5>Natalia Neverova</font></a><br>
<font size=2.5>In CVPR 2020 </font><br>
<a href="https://arxiv.org/abs/2003.00080"><img src="data/paper.png"></a> 
<a href="https://gdude.de/densepose-evolution/"><img src="data/project.png"></a>
</td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://elliottwu.com/projects/unsup3d/"><img src="teasers/Wu2020.jpg"/></a></td>
<td align="left" width=550><em>Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild</em><br>
<a href="https://elliottwu.com/"><font size=2.5>Shangzhe Wu</font></a>, 
<a href="https://chrirupp.github.io/"><font size=2.5>Christian Rupprecht</font></a>, 
<a href="http://www.robots.ox.ac.uk/~vedaldi/"><font size=2.5>Andrea Vedaldi</font></a><br>
<font size=2.5>In CVPR 2020 </font>(<b><font size=2.5>Best Paper Award</font></b>)<br>
<a href="https://arxiv.org/abs/1911.11130"><img src="data/paper.png"></a> 
<a href="https://elliottwu.com/projects/unsup3d/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-cat-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-cat-yellowgreen"></a><a href="https://img.shields.io/badge/topic-face-orange" align="bottom"><img src="https://img.shields.io/badge/topic-face-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/jgraving/deepposekit"><img src="teasers/Graving2019.jpg"/></a></td>
<td align="left" width=550><em>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</em><br>
<a href="https://jakegraving.com/files/cv/jacob_graving_cv.pdf"><font size=2.5>Jacob M. Graving</font></a>, 
<a href="https://www.danielchae.com/"><font size=2.5>Daniel Chae</font></a>, 
<font size=2.5>Hemal Naik</font>, 
<font size=2.5>Liang Li</font>, 
<font size=2.5>Benjamin Koger</font>, 
<a href="http://www.blaircostelloe.com/"><font size=2.5>Blair R. Costelloe</font></a>, 
<a href="http://collectivebehaviour.com/people/couzin-iain/#:~:text=Iain%20Couzin%20is%20Director%20of%20the%20Max%20Planck,Fellow%20in%20the%20Sciences%20at%20Balliol%20College%2C%20Oxford."><font size=2.5>Iain D. Couzin</font></a><br>
<font size=2.5>In eLife 2019 </font><br>
<a href="https://www.biorxiv.org/content/10.1101/620245v7"><img src="data/paper.png"></a> 
<a href="https://github.com/jgraving/deepposekit"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/DeepLabCut/DeepLabCut/blob/master/README.md"><img src="teasers/Nath2019.jpg"/></a></td>
<td align="left" width=550><em>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</em><br>
<font size=2.5>Tanmay Nath</font>, 
<a href="http://www.people.fas.harvard.edu/~amathis/"><font size=2.5>Alexander Mathis</font></a>, 
<font size=2.5>An Chi Chen</font>, 
<font size=2.5>Amir Patel</font>, 
<a href="http://bethgelab.org/"><font size=2.5>Matthias Bethge</font></a>, 
<a href="https://scholar.harvard.edu/mwamoroso/home"><font size=2.5>Mackenzie Weygandt Mathis</font></a><br>
<font size=2.5>In Nature Protocols 2019 </font>(<b><font size=2.5>cover</font></b>)<br>
<a href="https://www.nature.com/articles/s41596-019-0176-0"><img src="data/paper.png"></a> 
<a href="https://github.com/DeepLabCut/DeepLabCut/blob/master/README.md"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/talmo/leap"><img src="teasers/Pereira2019.jpg"/></a></td>
<td align="left" width=550><em>Fast animal pose estimation using deep neural networks</em><br>
<a href="https://biophysics.princeton.edu/people/talmo-pereira"><font size=2.5>Talmo D. Pereira</font></a>, 
<a href="https://olveczkylab.oeb.harvard.edu/people/diego-etiony-aldarondo"><font size=2.5>Diego E. Aldarondo</font></a>, 
<font size=2.5>Lindsay Willmore</font>, 
<a href="Mikhail Kislin"><font size=2.5>Mikhail Kislin</font></a>, 
<a href="https://scholar.princeton.edu/wanglab/people/samuel-s-h-wang"><font size=2.5>Samuel S.-H. Wang</font></a>, 
<a href="https://murthylab.princeton.edu/mala-murthy"><font size=2.5>Mala Murthy</font></a>, 
<a href="https://molbiod.princeton.edu/people/joshua-w-shaevitz"><font size=2.5>Joshua W. Shaevitz</font></a><br>
<font size=2.5>In Nature Methods 2019 </font><br>
<a href="https://www.nature.com/articles/s41592-018-0234-5"><img src="data/paper.png"></a> 
<a href="https://github.com/talmo/leap"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://edspace.american.edu/openbehavior/2018/12/05/live-mouse-tracker/"><img src="teasers/Chaumont2019.jpg"/></a></td>
<td align="left" width=550><em>Real-time analysis of the behaviour of groups of mice via a depth-sensing camera and machine learning</em><br>
<a href="https://research.pasteur.fr/en/member/fabrice-de-chaumont/"><font size=2.5>Fabrice de Chaumont</font></a>, 
<font size=2.5>Elodie Ey</font>, 
<font size=2.5>Nicolas Torquet</font>, 
<font size=2.5>Thibault Lagache</font>, 
<font size=2.5>Stéphane Dallongeville</font>, 
<font size=2.5>Albane Imbert</font>, 
<font size=2.5>Thierry Legou</font>, 
<font size=2.5>Anne-Marie Le Sourd</font>, 
<font size=2.5>Philippe Faure</font>, 
<a href="https://research.pasteur.fr/en/member/thomas-bourgeron/"><font size=2.5>Thomas Bourgeron</font></a>, 
<a href="https://research.pasteur.fr/en/member/jean-christophe-olivo-marin/"><font size=2.5>Jean-Christophe Olivo-Marin</font></a><br>
<font size=2.5>In Nature Biomedical Engineering 2019 </font><br>
<a href="https://www.nature.com/articles/s41551-019-0396-1"><img src="data/paper.png"></a> 
<a href="https://edspace.american.edu/openbehavior/2018/12/05/live-mouse-tracker/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://www.jinkuncao.com/animalpose"><img src="teasers/Cao2019.jpg"/></a></td>
<td align="left" width=550><em>Cross-Domain Adaptation for Animal Pose Estimation</em><br>
<a href="http://www.jinkuncao.com/"><font size=2.5>Jinkun Cao</font></a>, 
<font size=2.5>Hongyang Tang</font>, 
<a href="https://fang-haoshu.github.io/"><font size=2.5>Hao-Shu Fang</font></a>, 
<a href="http://xiaoyongshen.me/"><font size=2.5>Xiaoyong Shen</font></a>, 
<a href="https://www.mvig.org/"><font size=2.5>Cewu Lu</font></a>, 
<font size=2.5>Yu-Wing Tai</font><br>
<font size=2.5>In ICCV 2019 </font>(<b><font size=2.5>oral</font></b>)<br>
<a href="https://arxiv.org/abs/1908.05806"><img src="data/paper.png"></a> 
<a href="http://www.jinkuncao.com/animalpose"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://nileshkulkarni.github.io/csm/"><img src="teasers/Kulkarni2019.jpg"/></a></td>
<td align="left" width=550><em>Canonical Surface Mapping via Geometric Cycle Consistency</em><br>
<a href="https://nileshkulkarni.github.io/"><font size=2.5>Nilesh Kulkarni</font></a>, 
<a href="http://www.cs.cmu.edu/~abhinavg/"><font size=2.5>Abhinav Gupta</font></a>, 
<a href="https://shubhtuls.github.io/"><font size=2.5>Shubham Tulsiani</font></a><br>
<font size=2.5>In ICCV 2019 </font><br>
<a href="https://arxiv.org/pdf/1907.10043.pdf"><img src="data/paper.png"></a> 
<a href="https://nileshkulkarni.github.io/csm/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Yao2019.jpg"/></a></td>
<td align="left" width=550><em>MONET: Multiview Semi-supervised Keypoint Detection via Epipolar Divergence</em><br>
<font size=2.5>Yuan Yao</font>, 
<font size=2.5>Yasamin Jafarian</font>, 
<a href="https://www-users.cs.umn.edu/~hspark/"><font size=2.5>Hyun Soo Park</font></a><br>
<font size=2.5>In ICCV 2019 </font><br>
<a href="https://arxiv.org/abs/1806.00104"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-monkey-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-monkey-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/silviazuffi/smalst"><img src="teasers/Zuffi2019.jpg"/></a></td>
<td align="left" width=550><em>Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images "In the Wild"</em><br>
<a href="https://ps.is.tuebingen.mpg.de/person/szuffi"><font size=2.5>Silvia Zuffi</font></a>, 
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://www.cs.uic.edu/~tanyabw/"><font size=2.5>Tanya Berger-Wolf</font></a>, 
<a href="https://ps.is.tuebingen.mpg.de/person/black"><font size=2.5>Michael J. Black</font></a><br>
<font size=2.5>In ICCV 2019 </font><br>
<a href="https://arxiv.org/abs/1908.07201"><img src="data/paper.png"></a> 
<a href="https://github.com/silviazuffi/smalst"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://www.mousemotorlab.org/deeplabcut"><img src="teasers/Mathis2018.jpg"/></a></td>
<td align="left" width=550><em>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</em><br>
<a href="http://www.people.fas.harvard.edu/~amathis/"><font size=2.5>Alexander Mathis</font></a>, 
<font size=2.5>Pranav Mamidanna</font>, 
<a href="https://muckrack.com/kevin-m-cury"><font size=2.5>Kevin M. Cury</font></a>, 
<font size=2.5>Taiga Abe</font>, 
<a href="https://vnmurthylab.org/"><font size=2.5>Venkatesh N. Murthy</font></a>, 
<a href="https://scholar.harvard.edu/mwamoroso/home"><font size=2.5>Mackenzie Weygandt Mathis</font></a>, 
<a href="http://bethgelab.org/"><font size=2.5>Matthias Bethge</font></a><br>
<font size=2.5>In Nature Neuroscience 2018 </font><br>
<a href="https://www.nature.com/articles/s41593-018-0209-y"><img src="data/paper.png"></a> 
<a href="http://www.mousemotorlab.org/deeplabcut"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a><a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://akanazawa.github.io/cmr/"><img src="teasers/Kanazawa2018.jpg"/></a></td>
<td align="left" width=550><em>Learning Category-Specific Mesh Reconstruction from Image Collections</em><br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://shubhtuls.github.io/"><font size=2.5>Shubham Tulsiani</font></a>, 
<a href="https://people.eecs.berkeley.edu/~efros/"><font size=2.5>Alexei A. Efros</font></a>, 
<a href="https://people.eecs.berkeley.edu/~malik/"><font size=2.5>Jitendra Malik</font></a><br>
<font size=2.5>In ECCV 2018 </font><br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/papers/cmr_camera_ready.pdf"><img src="data/paper.png"></a> 
<a href="https://akanazawa.github.io/cmr/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://smalr.is.tue.mpg.de/"><img src="teasers/Zuffi2018.jpg"/></a></td>
<td align="left" width=550><em>Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</em><br>
<a href="https://ps.is.tuebingen.mpg.de/person/szuffi"><font size=2.5>Silvia Zuffi</font></a>, 
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://ps.is.tuebingen.mpg.de/person/black"><font size=2.5>Michael J. Black</font></a><br>
<font size=2.5>In CVPR 2018 </font>(<b><font size=2.5>spotlight</font></b>)<br>
<a href="http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf"><img src="data/paper.png"></a> 
<a href="http://smalr.is.tue.mpg.de/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Biggs2018.jpg"/></a></td>
<td align="left" width=550><em>Creatures great and SMAL: Recovering the shape and motion of animals from video</em><br>
<a href="http://mi.eng.cam.ac.uk/~bjb56/"><font size=2.5>Benjamin Biggs</font></a>, 
<a href="http://mi.eng.cam.ac.uk/~tr346/"><font size=2.5>Thomas Roddick</font></a>, 
<a href="https://www.microsoft.com/en-us/research/people/awf/"><font size=2.5>Andrew Fitzgibbon</font></a>, 
<a href="https://mi.eng.cam.ac.uk/~cipolla/"><font size=2.5>Roberto Cipolla</font></a><br>
<font size=2.5>In ACCV 2018 </font>(<b><font size=2.5>oral</font></b>)<br>
<a href="https://arxiv.org/abs/1811.05804"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Klibaite2017.jpg"/></a></td>
<td align="left" width=550><em>An unsupervised method for quantifying the behavior of paired animals</em><br>
<font size=2.5>Ugne Klibaite</font>, 
<font size=2.5>Gordon J Berman</font>, 
<font size=2.5>Jessica Cande</font>, 
<font size=2.5>David L Stern</font>, 
<a href="https://molbio.princeton.edu/people/joshua-w-shaevitz"><font size=2.5>Joshua W Shaevitz</font></a><br>
<font size=2.5>In Physical Biology 2017 </font><br>
<a href="https://doi.org/10.1088/1478-3975/aa5c50"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a><a href="https://img.shields.io/badge/topic-behavior-orange" align="bottom"><img src="https://img.shields.io/badge/topic-behavior-orange"></a><a href="https://img.shields.io/badge/animal-mouse-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-mouse-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Kanazawa2017.jpg"/></a></td>
<td align="left" width=550><em>Single-View 3D Reconstruction of Animals</em><br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a><br>
<font size=2.5>In Ph.D Thesis 2017 </font><br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/papers/thesis.pdf"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://smal.is.tue.mpg.de/"><img src="teasers/Zuffi2017.jpg"/></a></td>
<td align="left" width=550><em>3D Menagerie: Modeling the 3D shape and pose of animals</em><br>
<a href="https://ps.is.tuebingen.mpg.de/person/szuffi"><font size=2.5>Silvia Zuffi</font></a>, 
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://www.cs.umd.edu/~djacobs/"><font size=2.5>David W. Jacobs</font></a>, 
<a href="https://ps.is.tuebingen.mpg.de/person/black"><font size=2.5>Michael J. Black</font></a><br>
<font size=2.5>In CVPR 2017 </font>(<b><font size=2.5>spotlight</font></b>)<br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/papers/cvpr17_menagerie_camready.pdf"><img src="data/paper.png"></a> 
<a href="http://smal.is.tue.mpg.de/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-quadruped-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-quadruped-yellowgreen"></a><a href="https://img.shields.io/badge/topic-dataset-orange" align="bottom"><img src="https://img.shields.io/badge/topic-dataset-orange"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="none"><img src="teasers/Breslav2016.jpg"/></a></td>
<td align="left" width=550><em>3D Pose Estimation of Flying Animals in Multi-view Video Datasets</em><br>
<a href="http://people.bu.edu/breslav/"><font size=2.5>Mikhail Breslav</font></a><br>
<font size=2.5>In Ph.D Thesis 2016 </font><br>
<a href="https://open.bu.edu/handle/2144/19720"><img src="data/paper.png"></a> 
<a href="https://img.shields.io/badge/animal-bird-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-bird-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-3d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-3d-9cf"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="https://github.com/akanazawa/catdeform"><img src="teasers/Kanazawa2016.jpg"/></a></td>
<td align="left" width=550><em>Learning 3D Deformation of Animals from 2D Images</em><br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/"><font size=2.5>Angjoo Kanazawa</font></a>, 
<a href="https://shaharkov.github.io/"><font size=2.5>Shahar Kovalsky</font></a>, 
<a href="http://www.weizmann.ac.il/math/ronen/home"><font size=2.5>Ronen Basri</font></a>, 
<a href="https://www.cs.umd.edu/~djacobs/"><font size=2.5>David W. Jacobs</font></a><br>
<font size=2.5>In Eurographics 2016 </font>(<b><font size=2.5>Günter Enderle Best Paper Award</font></b>)<br>
<a href="https://people.eecs.berkeley.edu/~kanazawa/papers/cat_eg2016.pdf"><img src="data/paper.png"></a> 
<a href="https://github.com/akanazawa/catdeform"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/datatype-mesh-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-mesh-9cf"></a><a href="https://img.shields.io/badge/animal-cat-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-cat-yellowgreen"></a></td></tr></tbody>


<tbody> <tr> <td align="left" width=250>
<a href="http://www.idtracker.es/"><img src="teasers/Escudero2014.jpg"/></a></td>
<td align="left" width=550><em>idTracker: Tracking individuals in a group by automatic identification of unmarked animals</em><br>
<font size=2.5>Alfonso Pérez-Escudero</font>, 
<font size=2.5>Julián Vicente-Page</font>, 
<font size=2.5>Robert C Hinz</font>, 
<font size=2.5>Sara Arganda</font>, 
<a href="http://www.neuro.fchampalimaud.org/en/person/276/"><font size=2.5>Gonzalo G de Polavieja1</font></a><br>
<font size=2.5>In Nature Methods 2014 </font><br>
<a href="https://www.nature.com/articles/nmeth.2994"><img src="data/paper.png"></a> 
<a href="http://www.idtracker.es/"><img src="data/project.png"></a>
<a href="https://img.shields.io/badge/animal-fly-yellowgreen" align="bottom"><img src="https://img.shields.io/badge/animal-fly-yellowgreen"></a><a href="https://img.shields.io/badge/datatype-2d-9cf" align="bottom"><img src="https://img.shields.io/badge/datatype-2d-9cf"></a><a href="https://img.shields.io/badge/topic-social-orange" align="bottom"><img src="https://img.shields.io/badge/topic-social-orange"></a></td></tr></tbody>


</table>
<br>

Last updated in Feb 2024
<br>

This repository is inspired by [Cat Papers](https://github.com/junyanz/CatPapers) with some code borrowed from it. Paper and project signs are taken from [http://kesen.realtimerendering.com/](http://kesen.realtimerendering.com/). 