## Animal Paper Collection (Ongoing)
[2023.12.14] Here is a workshop involving animals and infants [CV4Smalls Workshop](https://cv4smalls.sites.northeastern.edu/accepted-submissions/)! Feel free to check it out!

[2023.11.26] My own paper "MAMMAL" is published online in Nature Communications now! 

[2023.06.10] The program of [CV4Animals Workshop in CVPR 2023](https://www.cv4animals.com/2023-accepted-papers) is available!

[2022.08.08] Here is the Ph.D thesis of SEMIH GÃœNEL (https://infoscience.epfl.ch/record/293821), who is the author of well-known DeepFly3D and LiftPose3D.

[2022.06.22] The 2nd CV4Animals Workshop in CVPR 2022 (https://www.cv4animals.com/) presents many excellent works on rodent 3D reconstruction!
  
[2022.01.02] To make it easy to track, I sort the papers using the timepoint I find them (not publication time), and add label badges to show paper features. 

[2021.11.28] I recommend [MMPose](https://github.com/open-mmlab/mmpose) for 2D animal pose estimation. It has collected various kinds of [datasets](https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_animal_keypoint.md).

[2021.07.16] I recommend CV4Animal workshop in CVPR 2021 (https://www.cv4animals.com/) because it is a good collection of recent advances in animal pose estimation area! 

Recently, markerless animal **motion capture** and **3D reconstruction** attracts more and more attention in computer vision community. Inspired by remarkable techniques for markerless human motion capture, a few excellent literatures appear for animal modeling and reconstruction such as [SMAL](http://smal.is.tue.mpg.de/
) and [DeepLabCut](http://www.mousemotorlab.org/deeplabcut). However, there is still a long way before computer vision methods could capture natural motion of arbitary animals in industrial-grade. 

Therefore, I contribute this repository to track every step towards the ultimate goal of high quality animal capture. If you want to add/remove an article, please send an email to [Liang An](https://anl13.github.io/)(al17 at mails dot tsinghua dot edu dot cn). Thank all the authors for their contribution and support.

